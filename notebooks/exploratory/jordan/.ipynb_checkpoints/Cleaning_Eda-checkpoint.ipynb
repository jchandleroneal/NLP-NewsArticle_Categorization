{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\">\n",
    "    <ul class=\"toc-item\">\n",
    "    <li><span><a href=\"#Cleaning the Data\" data-toc-modified-id=\"Cleaning the Data\">\n",
    "        <span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Cleaning the Data</a></span>    \n",
    "    <ul class=\"toc-item\">\n",
    "        <li><span><a href=\"#Importing the Data\" data-toc-modified-id=\"Importing the Data-1.1\">\n",
    "            <span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Importing the Data</a></span></li></ul></li>\n",
    "        <li><span><a href=\"#Inspecting the data\" data-toc-modified-id=\"Inspecting the data\">\n",
    "            <span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Inspecting the data</a></span>        \n",
    "    <ul class=\"toc-item\">\n",
    "        <li><span><a href=\"#Activation-functions-(from-before)\" data-toc-modified-id=\"Activation-functions-(from-before)-2.1\">\n",
    "              <span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Activation functions (from before)</a></span>  \n",
    "    <ul class=\"toc-item\">\n",
    "        <li><span><a href=\"#Recall-what-an-activation-function-does\" data-toc-modified-id=\"Recall-what-an-activation-function-does-2.1.1\">\n",
    "              <span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Recall what an activation function does</a></span></li>\n",
    "        <li><span><a href=\"#Using-the-proper-activation\" data-toc-modified-id=\"Using-the-proper-activation-2.1.2\">\n",
    "              <span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Using the proper activation</a></span></li></ul></li>\n",
    "        <li><span><a href=\"#Random-Starts\" data-toc-modified-id=\"Random-Starts-2.2\">\n",
    "              <span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Random Starts</a></span></li>\n",
    "        <li><span><a href=\"#Momentum\" data-toc-modified-id=\"Momentum-2.3\">\n",
    "              <span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Momentum</a></span>           \n",
    "    <ul class=\"toc-item\">\n",
    "        <li><span><a href=\"#Solution?--Give-it-some-speed-ðŸ˜Ž\" data-toc-modified-id=\"Solution?--Give-it-some-speed-ðŸ˜Ž-2.3.1\">\n",
    "              <span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>Solution?  Give it some speed ðŸ˜Ž</a></span><ul class=\"toc-item\">\n",
    "        <li><span><a href=\"#Code-Implementation\" data-toc-modified-id=\"Code-Implementation-2.3.1.1\">\n",
    "              <span class=\"toc-item-num\">2.3.1.1&nbsp;&nbsp;</span>Code Implementation</a></span></li></ul></li></ul></li></ul></li>\n",
    "        <li><span><a href=\"#Make-it-Go-Faster-with-Optimizers!\" data-toc-modified-id=\"Make-it-Go-Faster-with-Optimizers!-3\">\n",
    "              <span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Make it Go Faster with Optimizers!</a></span>            \n",
    "    <ul class=\"toc-item\">\n",
    "        <li><span><a href=\"#Normalization-(Batch-Normalization)\" data-toc-modified-id=\"Normalization-(Batch-Normalization)-3.1\">\n",
    "            <span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Normalization (Batch Normalization)</a></span></li>\n",
    "        <li><span><a href=\"#Stochastic-Gradient-Descent\" data-toc-modified-id=\"Stochastic-Gradient-Descent-3.2\">\n",
    "            <span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Stochastic Gradient Descent</a></span><ul class=\"toc-item\">\n",
    "        <li><span><a href=\"#Steps\" data-toc-modified-id=\"Steps-3.2.1\">\n",
    "            <span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Steps</a></span></li></ul></li>\n",
    "        <li><span><a href=\"#AdaGrad-&amp;-RMSProp\" data-toc-modified-id=\"AdaGrad-&amp;-RMSProp-3.3\">\n",
    "            <span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>AdaGrad &amp; RMSProp</a></span>                        \n",
    "    <ul class=\"toc-item\">\n",
    "        <li><span><a href=\"#Code-Example\" data-toc-modified-id=\"Code-Example-3.3.1\">\n",
    "            <span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>Code Example</a></span></li></ul></li>\n",
    "        <li><span><a href=\"#Adam-(Adaptivr-Moment-Estimation)-Optimization-and-Other-Variants\" data-toc-modified-id=\"Adam-(Adaptivr-Moment-\n",
    "                          Estimation)-Optimization-and-Other-Variants-3.4\">\n",
    "            <span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Adam (Adaptivr Moment Estimation) Optimization and Other Variants</a></span></li>          <li><span><a href=\"#Learning-Rate-Decay\" data-toc-modified-id=\"Learning-Rate-Decay-3.5\">\n",
    "            <span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Learning Rate Decay</a></span><ul class=\"toc-item\">\n",
    "        <li><span><a href=\"#Solution?\" data-toc-modified-id=\"Solution?-3.5.1\">\n",
    "            <span class=\"toc-item-num\">3.5.1&nbsp;&nbsp;</span>Solution?</a></span></li>\n",
    "        <li><span><a href=\"#Code-Example\" data-toc-modified-id=\"Code-Example-3.5.2\">\n",
    "            <span class=\"toc-item-num\">3.5.2&nbsp;&nbsp;</span>Code Example</a></span></li>\n",
    "</ul>\n",
    "</li>\n",
    "        \n",
    "</ul>\n",
    "</li>\n",
    "        \n",
    "</ul>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Cleaning the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>headline</th>\n",
       "      <th>authors</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CRIME</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>Melissa Jeltsen</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
       "      <td>She left her husband. He killed their children...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>Andy McDonald</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/will-smit...</td>\n",
       "      <td>Of course it has a song.</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 57</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/hugh-gran...</td>\n",
       "      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/jim-carre...</td>\n",
       "      <td>The actor gives Dems an ass-kicking for not fi...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/julianna-...</td>\n",
       "      <td>The \"Dietland\" actress said using the bags is ...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                           headline  \\\n",
       "0          CRIME  There Were 2 Mass Shootings In Texas Last Week...   \n",
       "1  ENTERTAINMENT  Will Smith Joins Diplo And Nicky Jam For The 2...   \n",
       "2  ENTERTAINMENT    Hugh Grant Marries For The First Time At Age 57   \n",
       "3  ENTERTAINMENT  Jim Carrey Blasts 'Castrato' Adam Schiff And D...   \n",
       "4  ENTERTAINMENT  Julianna Margulies Uses Donald Trump Poop Bags...   \n",
       "\n",
       "           authors                                               link  \\\n",
       "0  Melissa Jeltsen  https://www.huffingtonpost.com/entry/texas-ama...   \n",
       "1    Andy McDonald  https://www.huffingtonpost.com/entry/will-smit...   \n",
       "2       Ron Dicker  https://www.huffingtonpost.com/entry/hugh-gran...   \n",
       "3       Ron Dicker  https://www.huffingtonpost.com/entry/jim-carre...   \n",
       "4       Ron Dicker  https://www.huffingtonpost.com/entry/julianna-...   \n",
       "\n",
       "                                   short_description       date  \n",
       "0  She left her husband. He killed their children... 2018-05-26  \n",
       "1                           Of course it has a song. 2018-05-26  \n",
       "2  The actor and his longtime girlfriend Anna Ebe... 2018-05-26  \n",
       "3  The actor gives Dems an ass-kicking for not fi... 2018-05-26  \n",
       "4  The \"Dietland\" actress said using the bags is ... 2018-05-26  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd \n",
    "import imblearn\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline \n",
    "plt.style.use('fivethirtyeight')\n",
    "import nltk # importing Natural Landuage Processing Toolit\n",
    "from nltk.corpus import stopwords #importing stop words to remove non-necissary words\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer,CountVectorizer\n",
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from collections import Counter\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import re \n",
    "import nltk\n",
    "import string\n",
    "\n",
    "\n",
    "raw_data = pd.read_json('../../../data/data.json', lines=True)\n",
    "\n",
    "raw_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Inspecting the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200853 entries, 0 to 200852\n",
      "Data columns (total 6 columns):\n",
      " #   Column             Non-Null Count   Dtype         \n",
      "---  ------             --------------   -----         \n",
      " 0   category           200853 non-null  object        \n",
      " 1   headline           200853 non-null  object        \n",
      " 2   authors            200853 non-null  object        \n",
      " 3   link               200853 non-null  object        \n",
      " 4   short_description  200853 non-null  object        \n",
      " 5   date               200853 non-null  datetime64[ns]\n",
      "dtypes: datetime64[ns](1), object(5)\n",
      "memory usage: 9.2+ MB\n"
     ]
    }
   ],
   "source": [
    "#inspecting the data types - Noticed that they're all objects \n",
    "raw_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category             0\n",
       "headline             0\n",
       "authors              0\n",
       "link                 0\n",
       "short_description    0\n",
       "date                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking For Null Values\n",
    "raw_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>headline</th>\n",
       "      <th>authors</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CRIME</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>Melissa Jeltsen</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
       "      <td>She left her husband. He killed their children...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>Andy McDonald</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/will-smit...</td>\n",
       "      <td>Of course it has a song.</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 57</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/hugh-gran...</td>\n",
       "      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/jim-carre...</td>\n",
       "      <td>The actor gives Dems an ass-kicking for not fi...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/julianna-...</td>\n",
       "      <td>The \"Dietland\" actress said using the bags is ...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                           headline  \\\n",
       "0          CRIME  There Were 2 Mass Shootings In Texas Last Week...   \n",
       "1  ENTERTAINMENT  Will Smith Joins Diplo And Nicky Jam For The 2...   \n",
       "2  ENTERTAINMENT    Hugh Grant Marries For The First Time At Age 57   \n",
       "3  ENTERTAINMENT  Jim Carrey Blasts 'Castrato' Adam Schiff And D...   \n",
       "4  ENTERTAINMENT  Julianna Margulies Uses Donald Trump Poop Bags...   \n",
       "\n",
       "           authors                                               link  \\\n",
       "0  Melissa Jeltsen  https://www.huffingtonpost.com/entry/texas-ama...   \n",
       "1    Andy McDonald  https://www.huffingtonpost.com/entry/will-smit...   \n",
       "2       Ron Dicker  https://www.huffingtonpost.com/entry/hugh-gran...   \n",
       "3       Ron Dicker  https://www.huffingtonpost.com/entry/jim-carre...   \n",
       "4       Ron Dicker  https://www.huffingtonpost.com/entry/julianna-...   \n",
       "\n",
       "                                   short_description       date  \n",
       "0  She left her husband. He killed their children... 2018-05-26  \n",
       "1                           Of course it has a song. 2018-05-26  \n",
       "2  The actor and his longtime girlfriend Anna Ebe... 2018-05-26  \n",
       "3  The actor gives Dems an ass-kicking for not fi... 2018-05-26  \n",
       "4  The \"Dietland\" actress said using the bags is ... 2018-05-26  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reassigning dataframe to df to uphold the original structure of the data - used for later references.\n",
    "df = raw_data.copy()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the Data for Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Punctuation and cleaning rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Short Description Feature**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         She left her husband. He killed their children...\n",
       "1                                  Of course it has a song.\n",
       "2         The actor and his longtime girlfriend Anna Ebe...\n",
       "3         The actor gives Dems an ass-kicking for not fi...\n",
       "4         The \"Dietland\" actress said using the bags is ...\n",
       "                                ...                        \n",
       "200848    Verizon Wireless and AT&T are already promotin...\n",
       "200849    Afterward, Azarenka, more effusive with the pr...\n",
       "200850    Leading up to Super Bowl XLVI, the most talked...\n",
       "200851    CORRECTION: An earlier version of this story i...\n",
       "200852    The five-time all-star center tore into his te...\n",
       "Name: short_description, Length: 200853, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.short_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         There Were 2 Mass Shootings In Texas Last Week...\n",
       "1         Will Smith Joins Diplo And Nicky Jam For The 2...\n",
       "2           Hugh Grant Marries For The First Time At Age 57\n",
       "3         Jim Carrey Blasts 'Castrato' Adam Schiff And D...\n",
       "4         Julianna Margulies Uses Donald Trump Poop Bags...\n",
       "                                ...                        \n",
       "200848    RIM CEO Thorsten Heins' 'Significant' Plans Fo...\n",
       "200849    Maria Sharapova Stunned By Victoria Azarenka I...\n",
       "200850    Giants Over Patriots, Jets Over Colts Among  M...\n",
       "200851    Aldon Smith Arrested: 49ers Linebacker Busted ...\n",
       "200852    Dwight Howard Rips Teammates After Magic Loss ...\n",
       "Name: headline, Length: 200853, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.headline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing Casing and punctuaton**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    she left her husband he killed their children ...\n",
      "1                              of course it has a song\n",
      "Name: short_description, dtype: object\n",
      "\n",
      "============================================================\n",
      "\n",
      " 0    there were 2 mass shootings in texas last week...\n",
      "1    will smith joins diplo and nicky jam for the 2...\n",
      "Name: headline, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Removing punctuation. It helps us reduce the size of the data \n",
    "df['short_description'] = df['short_description'].str.lower().str.replace('[^\\w\\s]','')\n",
    "print(df['short_description'].head(2))\n",
    "\n",
    "print('\\n============================================================')\n",
    "\n",
    "df['headline'] = df['headline'].str.lower().str.replace('[^\\w\\s]','')\n",
    "print('\\n', df['headline'].head(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merging Description and Headline together**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* checking lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.short_description[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.headline[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#merging\n",
    "df['head_description'] = df.headline + ' ' + df.short_description\n",
    "len(df.head_description[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Removing Non-Contributional Words from the Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Jordan\n",
      "[nltk_data]     Johnson\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#downloading stopwords to use\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#inspecting stop words \n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>head_description</th>\n",
       "      <th>stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>there were 2 mass shootings in texas last week...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>will smith joins diplo and nicky jam for the 2...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hugh grant marries for the first time at age 5...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jim carrey blasts castrato adam schiff and dem...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>julianna margulies uses donald trump poop bags...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    head_description  stopwords\n",
       "0  there were 2 mass shootings in texas last week...         12\n",
       "1  will smith joins diplo and nicky jam for the 2...          8\n",
       "2  hugh grant marries for the first time at age 5...          9\n",
       "3  jim carrey blasts castrato adam schiff and dem...          7\n",
       "4  julianna margulies uses donald trump poop bags...          8"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how many stop words do we have? \n",
    "df['stopwords'] = df['head_description'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "df[['head_description','stopwords']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>headline</th>\n",
       "      <th>authors</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "      <th>date</th>\n",
       "      <th>head_description</th>\n",
       "      <th>stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CRIME</td>\n",
       "      <td>there were 2 mass shootings in texas last week...</td>\n",
       "      <td>Melissa Jeltsen</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
       "      <td>she left her husband he killed their children ...</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>there were 2 mass shootings in texas last week...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>will smith joins diplo and nicky jam for the 2...</td>\n",
       "      <td>Andy McDonald</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/will-smit...</td>\n",
       "      <td>of course it has a song</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>will smith joins diplo and nicky jam for the 2...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                           headline  \\\n",
       "0          CRIME  there were 2 mass shootings in texas last week...   \n",
       "1  ENTERTAINMENT  will smith joins diplo and nicky jam for the 2...   \n",
       "\n",
       "           authors                                               link  \\\n",
       "0  Melissa Jeltsen  https://www.huffingtonpost.com/entry/texas-ama...   \n",
       "1    Andy McDonald  https://www.huffingtonpost.com/entry/will-smit...   \n",
       "\n",
       "                                   short_description       date  \\\n",
       "0  she left her husband he killed their children ... 2018-05-26   \n",
       "1                            of course it has a song 2018-05-26   \n",
       "\n",
       "                                    head_description  stopwords  \n",
       "0  there were 2 mass shootings in texas last week...         12  \n",
       "1  will smith joins diplo and nicky jam for the 2...          8  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspecting stop words column to identify the number of unnecissary words within each column\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing Stop Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing stopwords \n",
    "df['clean_head_description'] = df['head_description'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2 mass shootings texas last week 1 tv left hus...\n",
       "1    smith joins diplo nicky jam 2018 world cups of...\n",
       "2    hugh grant marries first time age 57 actor lon...\n",
       "3    jim carrey blasts castrato adam schiff democra...\n",
       "4    julianna margulies uses donald trump poop bags...\n",
       "Name: clean_head_description, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_head_description'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing the Text to Up Sentences Into a List of Words:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **splitting a phrase, sentence, paragraph, or an entire text document into smaller units, such as individual words or terms. Each of these smaller units are called tokens.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmitizing the data to reduce repitition by removing the affixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiating a lemmatizer\n",
    "lemma = WordNetLemmatizer() #instantiate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing affixes from all words\n",
    "def lemm_words(data):\n",
    "    tokens = word_tokenize(data)   #Tokenizing data \n",
    "    lemmed_tokens = [lemma.lemmatize(word) for word in tokens]   #lemmatizing data\n",
    "    return ' '.join(lemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmed = df.head_description.apply(lemm_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = lemmed                              #there were 2 mass shooting in texas last week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing series into a list of lists \n",
    "df['unique_word'] = l.str.split(' ')   # [there, were, 2, mass, shooting, in, texas, la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Creating a Bag of Words for Vectorization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list containing all words \n",
    "\n",
    "unique_lemmed_list = []                #the  (appears ->)  261830\n",
    "for x in df['unique_word']:  \n",
    "    unique_lemmed_list += x  # append elements of lists to full list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Removing Single Characters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5855301"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#inspecting the original number of words\n",
    "len(unique_lemmed_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looping through and adding words > len(1) to a new list \n",
    "full_words = [] # creating quotes \n",
    "index = 0\n",
    "for words in unique_lemmed_list: \n",
    "    if len(words) > 1:\n",
    "        full_words.append(words)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5621546"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking the NEW number of words after removal\n",
    "len(full_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Changing New Bag of Words into a Series for TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "the             261830\n",
       "to              163957\n",
       "of              128119\n",
       "and             119623\n",
       "in               95541\n",
       "                 ...  \n",
       "cruyff               1\n",
       "schoolchoice         1\n",
       "auclair              1\n",
       "samoiloffs           1\n",
       "insolvent            1\n",
       "Length: 102078, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_value_counts = pd.Series(full_words).value_counts()\n",
    "word_value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>261830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>to</td>\n",
       "      <td>163957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>of</td>\n",
       "      <td>128119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>and</td>\n",
       "      <td>119623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>in</td>\n",
       "      <td>95541</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  word   count\n",
       "0  the  261830\n",
       "1   to  163957\n",
       "2   of  128119\n",
       "3  and  119623\n",
       "4   in   95541"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot = pd.DataFrame(data = word_value_counts).reset_index()\n",
    "plot.columns = ['word','count']\n",
    "plot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['POLITICS', 'WELLNESS', 'ENTERTAINMENT', 'TRAVEL', 'STYLE & BEAUTY',\n",
       "       'PARENTING', 'HEALTHY LIVING', 'QUEER VOICES', 'FOOD & DRINK',\n",
       "       'BUSINESS', 'COMEDY', 'SPORTS', 'BLACK VOICES', 'HOME & LIVING',\n",
       "       'PARENTS', 'THE WORLDPOST', 'WEDDINGS', 'WOMEN', 'IMPACT', 'DIVORCE',\n",
       "       'CRIME', 'MEDIA', 'WEIRD NEWS', 'GREEN', 'WORLDPOST', 'RELIGION',\n",
       "       'STYLE', 'SCIENCE', 'WORLD NEWS', 'TASTE', 'TECH', 'MONEY', 'ARTS',\n",
       "       'FIFTY', 'GOOD NEWS', 'ARTS & CULTURE', 'ENVIRONMENT', 'COLLEGE',\n",
       "       'LATINO VOICES', 'CULTURE & ARTS', 'EDUCATION'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = df['category'].value_counts().index\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupper(grouplist,name):\n",
    "    for idx in categories: #for each category name\n",
    "        if idx in grouplist: #if the category name appears in the grouplist below\n",
    "            df.loc[df['category'] == idx, 'category'] = name  #assign it to name = \" \"\n",
    "groupper( grouplist= ['WELLNESS', 'HEALTHY LIVING','HOME & LIVING','STYLE & BEAUTY' ,'STYLE'] , name =  'LIFESTYLE AND WELLNESS')\n",
    "groupper( grouplist= [ 'PARENTING', 'PARENTS' ,'EDUCATION' ,'COLLEGE'] , name =  'PARENTING AND EDUCATION')\n",
    "groupper( grouplist= ['SPORTS','ENTERTAINMENT' , 'COMEDY','WEIRD NEWS','ARTS'] , name =  'SPORTS AND ENTERTAINMENT')\n",
    "groupper( grouplist= ['TRAVEL', 'ARTS & CULTURE','CULTURE & ARTS','FOOD & DRINK', 'TASTE'] , name =  'TRAVEL-TOURISM & ART-CULTURE')\n",
    "groupper( grouplist= ['WOMEN','QUEER VOICES', 'LATINO VOICES', 'BLACK VOICES'] , name =  'EMPOWERED VOICES')\n",
    "groupper( grouplist= ['BUSINESS' ,  'MONEY'] , name =  'BUSINESS-MONEY')\n",
    "groupper( grouplist= ['THE WORLDPOST' , 'WORLDPOST' , 'WORLD NEWS'] , name =  'WORLDNEWS')\n",
    "groupper( grouplist= ['ENVIRONMENT' ,'GREEN'] , name =  'ENVIRONMENT')\n",
    "groupper( grouplist= ['TECH', 'SCIENCE'] , name =  'SCIENCE AND TECH')\n",
    "groupper( grouplist= ['FIFTY' , 'IMPACT' ,'GOOD NEWS','CRIME'] , name =  'GENERAL')\n",
    "groupper( grouplist= ['WEDDINGS', 'DIVORCE',  'RELIGION','MEDIA'] , name =  'MISC')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = plot.word\n",
    "count = plot.count\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,18))\n",
    "\n",
    "plot.plot.barh(x='word', y= plot.count,\n",
    "                      ax=ax,\n",
    "                      color=\"purple\")\n",
    "\n",
    "# plot.word.value_counts()[:20].sort_index(ascending=False).plot(kind='barh')\n",
    "\n",
    "\n",
    "# ax.set_title(\"Common Words Found in DS Job Descriptions(Without Stop Words)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **TF - IDF**\n",
    "\n",
    "**Term Frequency â€” Inverse Document Frequency** - Derermining word importance to documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    [will, smith, join, diplo, and, nicky, jam, fo...\n",
       "2    [hugh, grant, marries, for, the, first, time, ...\n",
       "3    [jim, carrey, blast, castrato, adam, schiff, a...\n",
       "4    [julianna, margulies, us, donald, trump, poop,...\n",
       "5    [morgan, freeman, devastated, that, sexual, ha...\n",
       "6    [donald, trump, is, lovin, new, mcdonalds, jin...\n",
       "7    [what, to, watch, on, amazon, prime, thats, ne...\n",
       "Name: head_description, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#inspecting lemmatized words \n",
    "l[1:8].str.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Vectorizing the (title description + full description) column (converting it into numeric data).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a function to pass into the analyzer to clean the raw data \n",
    "    \n",
    "def clean_history(history):\n",
    "    history = \"\".join([word for word in history if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', history)\n",
    "    history = [wn.lemmatize(word) for word in tokens if word not in stopwords]\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returning Trigrams - Groups of Three Words\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf = tfidf_vectorizer.fit_transform(df['head_description'])\n",
    "features = (tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Shape of tfidf : \n",
      " (200853, 1600)\n",
      "\n",
      "\n",
      "Features : \n",
      " ['10', '100', '11', '12', '13', '14', '15', '16', '17', '18', '20', '2012', '2013', '2014', '2015', '2016', '2017', '25', '30', '40', '50', 'ability', 'able', 'about', 'above', 'absolutely', 'abuse', 'access', 'according', 'accused', 'across', 'act', 'action', 'activists', 'actor', 'actress', 'actually', 'ad', 'add', 'address', 'administration', 'adorable', 'adults', 'advice', 'after', 'again', 'against', 'age', 'ago', 'agree', 'ahead', 'air', 'airlines', 'airport', 'album', 'alive', 'all', 'allegedly', 'allow', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'amazing', 'america', 'american', 'americans', 'americas', 'amid', 'among', 'an', 'and', 'angeles', 'animal', 'animals', 'anniversary', 'announced', 'annual', 'another', 'answer', 'anxiety', 'any', 'anyone', 'anything', 'apparently', 'apple', 'approach', 'april', 'are', 'area', 'arent', 'around', 'arrested', 'art', 'article', 'artist', 'artists', 'as', 'ask', 'asked', 'asking', 'assault', 'at', 'attack', 'attacks', 'attention', 'attorney', 'author', 'autism', 'avoid', 'awards', 'awareness', 'away', 'awesome', 'babies', 'baby', 'back', 'bad', 'balance', 'ban', 'bank', 'bar', 'based', 'battle', 'be', 'beach', 'beat', 'beautiful', 'beauty', 'became', 'because', 'become', 'becomes', 'becoming', 'bed', 'been', 'beer', 'before', 'began', 'begin', 'beginning', 'behavior', 'behind', 'being', 'believe', 'below', 'ben', 'benefits', 'bernie', 'best', 'better', 'between', 'beyond', 'big', 'biggest', 'bill', 'billion', 'birth', 'birthday', 'bit', 'black', 'blood', 'blue', 'board', 'body', 'book', 'books', 'born', 'both', 'bowl', 'box', 'boy', 'boys', 'brain', 'brand', 'break', 'breakfast', 'breaking', 'breast', 'bring', 'brings', 'british', 'broken', 'brought', 'brown', 'budget', 'build', 'building', 'bush', 'business', 'but', 'buy', 'by', 'cake', 'california', 'call', 'called', 'calling', 'calls', 'came', 'campaign', 'can', 'cancer', 'candidate', 'candidates', 'cannot', 'cant', 'car', 'card', 'care', 'career', 'carolina', 'carpet', 'case', 'cases', 'cast', 'cat', 'caught', 'cause', 'celebrate', 'celebrating', 'celebrities', 'celebrity', 'center', 'central', 'century', 'ceo', 'certain', 'certainly', 'challenge', 'challenges', 'chance', 'change', 'changed', 'changes', 'changing', 'character', 'charged', 'check', 'cheese', 'chicago', 'chicken', 'chief', 'child', 'childhood', 'children', 'childrens', 'china', 'chinese', 'chocolate', 'choice', 'choices', 'choose', 'chris', 'christian', 'christmas', 'church', 'cities', 'city', 'civil', 'claim', 'claims', 'class', 'classic', 'clean', 'clear', 'click', 'climate', 'clinton', 'close', 'coast', 'coffee', 'colbert', 'cold', 'collection', 'college', 'color', 'come', 'comedy', 'comes', 'coming', 'comments', 'committee', 'common', 'communities', 'community', 'companies', 'company', 'completely', 'congress', 'conservative', 'consider', 'continue', 'continues', 'control', 'controversial', 'conversation', 'cooking', 'cool', 'cops', 'cost', 'costs', 'could', 'couldnt', 'countries', 'country', 'couple', 'couples', 'course', 'court', 'cover', 'crazy', 'cream', 'create', 'created', 'creating', 'creative', 'credit', 'crime', 'crisis', 'critical', 'cruz', 'culture', 'cup', 'current', 'cut', 'cute', 'dad', 'dads', 'daily', 'dance', 'dangerous', 'dark', 'data', 'date', 'dating', 'daughter', 'daughters', 'david', 'day', 'days', 'dc', 'de', 'dead', 'deadly', 'deal', 'dear', 'death', 'debate', 'debt', 'decades', 'decided', 'decision', 'deep', 'defense', 'definitely', 'delicious', 'democratic', 'democrats', 'department', 'depression', 'design', 'designer', 'despite', 'details', 'development', 'did', 'didnt', 'die', 'died', 'dies', 'diet', 'difference', 'different', 'difficult', 'digital', 'dinner', 'director', 'disease', 'disney', 'divorce', 'diy', 'do', 'doctor', 'doctors', 'does', 'doesnt', 'dog', 'dogs', 'doing', 'donald', 'done', 'dont', 'door', 'down', 'dr', 'dream', 'dreams', 'dress', 'drink', 'drinking', 'drive', 'drug', 'drugs', 'due', 'during', 'each', 'earlier', 'early', 'earth', 'easier', 'east', 'easy', 'eat', 'eating', 'economic', 'economy', 'education', 'effect', 'effects', 'effort', 'efforts', 'either', 'election', 'elizabeth', 'else', 'email', 'emotional', 'employees', 'end', 'energy', 'enjoy', 'enough', 'entire', 'episode', 'equality', 'especially', 'europe', 'even', 'event', 'events', 'ever', 'every', 'everyone', 'everything', 'everywhere', 'evidence', 'exactly', 'executive', 'exercise', 'expect', 'expected', 'experience', 'experiences', 'experts', 'explain', 'explains', 'extra', 'eye', 'eyes', 'face', 'facebook', 'faces', 'facing', 'fact', 'facts', 'fail', 'fair', 'faith', 'fake', 'fall', 'families', 'family', 'famous', 'fan', 'fans', 'far', 'fashion', 'fast', 'fat', 'father', 'fathers', 'favorite', 'fbi', 'fear', 'federal', 'feel', 'feeling', 'feelings', 'feels', 'felt', 'female', 'festival', 'few', 'field', 'fight', 'fighting', 'film', 'final', 'finally', 'financial', 'find', 'finding', 'finds', 'fine', 'fire', 'fired', 'first', 'fit', 'fitness', 'five', 'fix', 'flight', 'florida', 'focus', 'follow', 'following', 'food', 'foods', 'football', 'for', 'force', 'forces', 'foreign', 'forever', 'forget', 'form', 'former', 'forward', 'found', 'four', 'fox', 'free', 'freedom', 'french', 'fresh', 'friday', 'friend', 'friends', 'from', 'front', 'full', 'fun', 'funny', 'further', 'future', 'game', 'games', 'gave', 'gay', 'gender', 'general', 'generation', 'george', 'get', 'gets', 'getting', 'gift', 'gifts', 'girl', 'girls', 'give', 'given', 'gives', 'giving', 'global', 'go', 'goal', 'goals', 'god', 'goes', 'going', 'gold', 'golden', 'gone', 'good', 'google', 'gop', 'gorgeous', 'got', 'government', 'governor', 'gps', 'great', 'greatest', 'green', 'ground', 'group', 'groups', 'grow', 'growing', 'growth', 'guide', 'gun', 'guns', 'guy', 'guys', 'habits', 'had', 'hair', 'half', 'halloween', 'hand', 'hands', 'happen', 'happened', 'happens', 'happiness', 'happy', 'harassment', 'hard', 'harry', 'has', 'hate', 'have', 'havent', 'having', 'he', 'head', 'health', 'healthy', 'hear', 'heard', 'heart', 'held', 'hell', 'help', 'helped', 'helping', 'helps', 'her', 'here', 'heres', 'hes', 'high', 'higher', 'hilarious', 'hill', 'hillary', 'him', 'himself', 'his', 'historic', 'history', 'hit', 'hits', 'hold', 'holiday', 'holidays', 'hollywood', 'home', 'homeless', 'homes', 'honor', 'hope', 'hopes', 'hospital', 'host', 'hot', 'hotel', 'hotels', 'hours', 'house', 'how', 'however', 'huffpost', 'huge', 'human', 'hundreds', 'hurricane', 'hurt', 'husband', 'ice', 'id', 'idea', 'ideas', 'if', 'ill', 'illness', 'im', 'image', 'images', 'imagine', 'immigration', 'impact', 'important', 'improve', 'in', 'include', 'including', 'increase', 'incredible', 'industry', 'information', 'inner', 'inside', 'inspiration', 'inspired', 'instagram', 'instead', 'insurance', 'interest', 'international', 'internet', 'interview', 'into', 'investigation', 'involved', 'iran', 'iraq', 'is', 'isis', 'island', 'isnt', 'israel', 'issue', 'issues', 'it', 'its', 'itself', 'ive', 'james', 'jeff', 'jenner', 'jennifer', 'jessica', 'jimmy', 'job', 'jobs', 'joe', 'john', 'join', 'jones', 'journey', 'joy', 'judge', 'july', 'june', 'just', 'justice', 'justin', 'kardashian', 'kate', 'keep', 'keeping', 'kelly', 'key', 'kid', 'kids', 'kill', 'killed', 'killing', 'kim', 'kind', 'king', 'kitchen', 'knew', 'know', 'known', 'knows', 'korea', 'la', 'labor', 'lack', 'lady', 'land', 'large', 'largest', 'last', 'late', 'later', 'latest', 'law', 'lawmakers', 'laws', 'lawsuit', 'lawyer', 'lead', 'leader', 'leaders', 'leadership', 'leading', 'learn', 'learned', 'learning', 'least', 'leave', 'leaves', 'leaving', 'led', 'left', 'legacy', 'legal', 'less', 'lesson', 'lessons', 'let', 'lets', 'letter', 'level', 'lgbt', 'lgbtq', 'life', 'light', 'like', 'likely', 'line', 'linked', 'list', 'listen', 'little', 'live', 'lives', 'living', 'local', 'london', 'long', 'longer', 'look', 'looked', 'looking', 'looks', 'los', 'lose', 'losing', 'loss', 'lost', 'lot', 'love', 'loved', 'loves', 'loving', 'low', 'lower', 'made', 'magazine', 'magic', 'major', 'majority', 'make', 'makes', 'makeup', 'making', 'male', 'man', 'many', 'march', 'mark', 'market', 'marriage', 'married', 'martin', 'mass', 'massive', 'matter', 'matters', 'may', 'maybe', 'me', 'mean', 'meaning', 'means', 'media', 'medical', 'medicine', 'meditation', 'meet', 'meeting', 'members', 'men', 'mental', 'message', 'met', 'mexico', 'michael', 'michelle', 'middle', 'might', 'mike', 'military', 'million', 'millions', 'mind', 'minutes', 'miss', 'missing', 'mistakes', 'model', 'models', 'modern', 'mom', 'moment', 'moments', 'moms', 'monday', 'money', 'month', 'months', 'more', 'morning', 'most', 'mother', 'mothers', 'move', 'movement', 'movie', 'movies', 'moving', 'much', 'murder', 'music', 'muslim', 'must', 'my', 'myself', 'name', 'named', 'names', 'nation', 'national', 'nations', 'natural', 'nature', 'near', 'nearly', 'need', 'needed', 'needs', 'network', 'never', 'new', 'news', 'next', 'nfl', 'nice', 'night', 'no', 'nominee', 'normal', 'north', 'not', 'note', 'nothing', 'november', 'now', 'nuclear', 'number', 'numbers', 'obama', 'obamacare', 'obamas', 'of', 'off', 'offer', 'offers', 'office', 'officer', 'officers', 'official', 'officials', 'often', 'oh', 'oil', 'ok', 'old', 'older', 'olympic', 'olympics', 'on', 'once', 'one', 'ones', 'online', 'only', 'open', 'opening', 'opens', 'opportunity', 'options', 'or', 'order', 'oscar', 'oscars', 'other', 'others', 'our', 'ourselves', 'out', 'outside', 'over', 'own', 'paid', 'pain', 'paper', 'parent', 'parenting', 'parents', 'paris', 'park', 'part', 'partner', 'party', 'pass', 'past', 'path', 'patients', 'paul', 'pay', 'peace', 'people', 'percent', 'perfect', 'performance', 'perhaps', 'person', 'personal', 'phone', 'photo', 'photos', 'physical', 'pick', 'picture', 'piece', 'pinterest', 'place', 'places', 'plan', 'plane', 'planet', 'planned', 'planning', 'plans', 'play', 'player', 'players', 'playing', 'please', 'plenty', 'point', 'points', 'police', 'policy', 'political', 'politics', 'poll', 'poor', 'pop', 'pope', 'popular', 'positive', 'possible', 'post', 'potential', 'pounds', 'power', 'powerful', 'practice', 'pregnancy', 'pregnant', 'present', 'president', 'presidential', 'presidents', 'press', 'pressure', 'pretty', 'prevent', 'price', 'primary', 'prince', 'princess', 'prison', 'private', 'probably', 'problem', 'problems', 'process', 'products', 'program', 'project', 'promise', 'proposal', 'protect', 'protest', 'prove', 'provide', 'public', 'published', 'push', 'put', 'putting', 'quality', 'queen', 'queer', 'question', 'questions', 'quick', 'quickly', 'quite', 'race', 'racist', 'raise', 'raising', 'rape', 'rare', 'rate', 'rather', 'reach', 'read', 'reading', 'ready', 'real', 'reality', 'realize', 'really', 'reason', 'reasons', 'received', 'recent', 'recently', 'recipe', 'recipes', 'record', 'red', 'reform', 'refugees', 'relationship', 'relationships', 'release', 'released', 'religious', 'remember', 'rep', 'report', 'reported', 'reportedly', 'reports', 'republican', 'republicans', 'research', 'researchers', 'respect', 'response', 'responsibility', 'rest', 'restaurant', 'result', 'results', 'return', 'reuters', 'reveals', 'review', 'rich', 'ride', 'right', 'rights', 'ring', 'rise', 'risk', 'road', 'robert', 'rock', 'role', 'room', 'round', 'roundup', 'rule', 'rules', 'run', 'running', 'russia', 'russian', 'ryan', 'sad', 'safe', 'safety', 'said', 'same', 'san', 'sanders', 'saturday', 'save', 'saw', 'say', 'saying', 'says', 'scandal', 'scene', 'school', 'schools', 'science', 'scientists', 'scott', 'sea', 'search', 'season', 'second', 'secret', 'secretary', 'secrets', 'security', 'see', 'seeing', 'seem', 'seems', 'seen', 'self', 'sen', 'senate', 'senator', 'send', 'sense', 'sent', 'series', 'serious', 'seriously', 'serve', 'service', 'set', 'seven', 'several', 'sex', 'sexual', 'share', 'shared', 'shares', 'she', 'shes', 'shoes', 'shooting', 'shop', 'shopping', 'short', 'shot', 'should', 'shouldnt', 'show', 'shows', 'sick', 'side', 'sign', 'signs', 'similar', 'simple', 'simply', 'since', 'singer', 'single', 'site', 'situation', 'six', 'skin', 'sleep', 'slideshow', 'small', 'smart', 'smith', 'snl', 'so', 'social', 'society', 'some', 'someone', 'something', 'sometimes', 'son', 'song', 'soon', 'soul', 'sound', 'sounds', 'south', 'space', 'speak', 'special', 'speech', 'spend', 'spending', 'spent', 'spirit', 'spiritual', 'sports', 'spot', 'spring', 'st', 'staff', 'stage', 'stand', 'star', 'stars', 'start', 'started', 'starting', 'state', 'statement', 'states', 'stay', 'step', 'stephen', 'steps', 'steve', 'still', 'stop', 'store', 'stories', 'storm', 'story', 'straight', 'strategy', 'street', 'stress', 'strong', 'struggle', 'student', 'students', 'study', 'stuff', 'stunning', 'style', 'success', 'successful', 'such', 'sugar', 'suggests', 'suicide', 'summer', 'sunday', 'super', 'support', 'supporters', 'supreme', 'sure', 'surprise', 'surprising', 'survey', 'suspect', 'sweet', 'syria', 'system', 'table', 'take', 'taken', 'takes', 'taking', 'talk', 'talking', 'talks', 'taste', 'taught', 'tax', 'taylor', 'teach', 'teacher', 'teachers', 'team', 'tech', 'technology', 'ted', 'teen', 'teens', 'tell', 'telling', 'tells', 'test', 'texas', 'than', 'thank', 'thanks', 'thanksgiving', 'that', 'thats', 'the', 'their', 'them', 'themselves', 'then', 'there', 'theres', 'these', 'they', 'theyre', 'thing', 'things', 'think', 'thinking', 'thinks', 'third', 'this', 'those', 'though', 'thought', 'thoughts', 'thousands', 'threat', 'three', 'through', 'throughout', 'thursday', 'time', 'times', 'tips', 'to', 'today', 'todays', 'together', 'told', 'tom', 'too', 'took', 'top', 'totally', 'touch', 'tough', 'tour', 'toward', 'town', 'track', 'trade', 'trailer', 'train', 'training', 'trans', 'transgender', 'travel', 'travelers', 'traveling', 'treat', 'treatment', 'trend', 'trends', 'tribute', 'tried', 'trip', 'true', 'truly', 'trump', 'trumps', 'trust', 'truth', 'try', 'trying', 'tuesday', 'tumblr', 'turkey', 'turn', 'turned', 'turning', 'turns', 'tv', 'tweet', 'tweets', 'twitter', 'two', 'type', 'ultimate', 'un', 'under', 'understand', 'understanding', 'union', 'unique', 'united', 'university', 'until', 'up', 'update', 'upon', 'us', 'use', 'used', 'uses', 'using', 'usually', 'vacation', 'valentines', 'value', 'version', 'very', 'victims', 'victory', 'video', 'videos', 'view', 'vintage', 'violence', 'visit', 'voice', 'vote', 'voters', 'voting', 'vs', 'wait', 'waiting', 'wake', 'walk', 'walking', 'wall', 'want', 'wanted', 'wants', 'war', 'warm', 'wars', 'was', 'washington', 'wasnt', 'watch', 'watching', 'water', 'way', 'ways', 'we', 'wear', 'wearing', 'weather', 'wedding', 'weddings', 'wednesday', 'week', 'weekend', 'weekly', 'weeks', 'weight', 'weird', 'welcome', 'well', 'went', 'were', 'west', 'weve', 'what', 'whatever', 'whats', 'when', 'where', 'whether', 'which', 'while', 'white', 'who', 'whole', 'whos', 'whose', 'why', 'wife', 'wild', 'will', 'williams', 'win', 'wine', 'wins', 'winter', 'wisdom', 'wish', 'with', 'within', 'without', 'woman', 'women', 'womens', 'won', 'wonder', 'wont', 'word', 'words', 'work', 'workers', 'working', 'workout', 'works', 'world', 'worlds', 'worry', 'worse', 'worst', 'worth', 'would', 'wouldnt', 'write', 'writing', 'wrong', 'wrote', 'year', 'years', 'yes', 'yet', 'yoga', 'york', 'you', 'youll', 'young', 'your', 'youre', 'yourself', 'youth', 'youve']\n",
      "\n",
      "\n",
      "X1 : \n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\nShape of tfidf : \\n\", X_tfidf.shape)\n",
    "\n",
    "\n",
    "#printing out the features \n",
    "print(\"\\n\\nFeatures : \\n\", features)\n",
    "print(\"\\n\\nX1 : \\n\", X_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10 of the</th>\n",
       "      <th>10 things you</th>\n",
       "      <th>10 ways to</th>\n",
       "      <th>20 funniest tweets</th>\n",
       "      <th>247 wall st</th>\n",
       "      <th>4th of july</th>\n",
       "      <th>about donald trump</th>\n",
       "      <th>about how to</th>\n",
       "      <th>about them in</th>\n",
       "      <th>according to new</th>\n",
       "      <th>...</th>\n",
       "      <th>your life and</th>\n",
       "      <th>your new years</th>\n",
       "      <th>your wedding day</th>\n",
       "      <th>your weekly travel</th>\n",
       "      <th>your wellbeing off</th>\n",
       "      <th>youre going to</th>\n",
       "      <th>youre looking for</th>\n",
       "      <th>youre looking to</th>\n",
       "      <th>youtube videos of</th>\n",
       "      <th>youve never seen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200848</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200849</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200850</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200851</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200852</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200853 rows Ã— 1600 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        10 of the  10 things you  10 ways to  20 funniest tweets  247 wall st  \\\n",
       "0             0.0            0.0         0.0                 0.0          0.0   \n",
       "1             0.0            0.0         0.0                 0.0          0.0   \n",
       "2             0.0            0.0         0.0                 0.0          0.0   \n",
       "3             0.0            0.0         0.0                 0.0          0.0   \n",
       "4             0.0            0.0         0.0                 0.0          0.0   \n",
       "...           ...            ...         ...                 ...          ...   \n",
       "200848        0.0            0.0         0.0                 0.0          0.0   \n",
       "200849        0.0            0.0         0.0                 0.0          0.0   \n",
       "200850        0.0            0.0         0.0                 0.0          0.0   \n",
       "200851        0.0            0.0         0.0                 0.0          0.0   \n",
       "200852        0.0            0.0         0.0                 0.0          0.0   \n",
       "\n",
       "        4th of july  about donald trump  about how to  about them in  \\\n",
       "0               0.0                 0.0           0.0            0.0   \n",
       "1               0.0                 0.0           0.0            0.0   \n",
       "2               0.0                 0.0           0.0            0.0   \n",
       "3               0.0                 0.0           0.0            0.0   \n",
       "4               0.0                 0.0           0.0            0.0   \n",
       "...             ...                 ...           ...            ...   \n",
       "200848          0.0                 0.0           0.0            0.0   \n",
       "200849          0.0                 0.0           0.0            0.0   \n",
       "200850          0.0                 0.0           0.0            0.0   \n",
       "200851          0.0                 0.0           0.0            0.0   \n",
       "200852          0.0                 0.0           0.0            0.0   \n",
       "\n",
       "        according to new  ...  your life and  your new years  \\\n",
       "0                    0.0  ...            0.0             0.0   \n",
       "1                    0.0  ...            0.0             0.0   \n",
       "2                    0.0  ...            0.0             0.0   \n",
       "3                    0.0  ...            0.0             0.0   \n",
       "4                    0.0  ...            0.0             0.0   \n",
       "...                  ...  ...            ...             ...   \n",
       "200848               0.0  ...            0.0             0.0   \n",
       "200849               0.0  ...            0.0             0.0   \n",
       "200850               0.0  ...            0.0             0.0   \n",
       "200851               0.0  ...            0.0             0.0   \n",
       "200852               0.0  ...            0.0             0.0   \n",
       "\n",
       "        your wedding day  your weekly travel  your wellbeing off  \\\n",
       "0                    0.0                 0.0                 0.0   \n",
       "1                    0.0                 0.0                 0.0   \n",
       "2                    0.0                 0.0                 0.0   \n",
       "3                    0.0                 0.0                 0.0   \n",
       "4                    0.0                 0.0                 0.0   \n",
       "...                  ...                 ...                 ...   \n",
       "200848               0.0                 0.0                 0.0   \n",
       "200849               0.0                 0.0                 0.0   \n",
       "200850               0.0                 0.0                 0.0   \n",
       "200851               0.0                 0.0                 0.0   \n",
       "200852               0.0                 0.0                 0.0   \n",
       "\n",
       "        youre going to  youre looking for  youre looking to  \\\n",
       "0                  0.0                0.0               0.0   \n",
       "1                  0.0                0.0               0.0   \n",
       "2                  0.0                0.0               0.0   \n",
       "3                  0.0                0.0               0.0   \n",
       "4                  0.0                0.0               0.0   \n",
       "...                ...                ...               ...   \n",
       "200848             0.0                0.0               0.0   \n",
       "200849             0.0                0.0               0.0   \n",
       "200850             0.0                0.0               0.0   \n",
       "200851             0.0                0.0               0.0   \n",
       "200852             0.0                0.0               0.0   \n",
       "\n",
       "        youtube videos of  youve never seen  \n",
       "0                     0.0               0.0  \n",
       "1                     0.0               0.0  \n",
       "2                     0.0               0.0  \n",
       "3                     0.0               0.0  \n",
       "4                     0.0               0.0  \n",
       "...                   ...               ...  \n",
       "200848                0.0               0.0  \n",
       "200849                0.0               0.0  \n",
       "200850                0.0               0.0  \n",
       "200851                0.0               0.0  \n",
       "200852                0.0               0.0  \n",
       "\n",
       "[200853 rows x 1600 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X_tfidf_df\n",
    "y=df.category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = scaler.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_test, y_tr, y_test = train_test_split(X, y, test_size=20, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_tr, y_tr, test_size=20, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LIFESTYLE AND WELLNESS          40609\n",
       "POLITICS                        32730\n",
       "SPORTS AND ENTERTAINMENT        30291\n",
       "TRAVEL-TOURISM & ART-CULTURE    20571\n",
       "EMPOWERED VOICES                15458\n",
       "PARENTING AND EDUCATION         14778\n",
       "MISC                            12447\n",
       "GENERAL                          9661\n",
       "WORLDNEWS                        8420\n",
       "BUSINESS-MONEY                   7644\n",
       "SCIENCE AND TECH                 4259\n",
       "ENVIRONMENT                      3945\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe1 = make_pipeline(SMOTE(random_state=3),DecisionTreeClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe2 = make_pipeline(SMOTE(random_state=3),RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe3 = make_pipeline(SMOTE(random_state=3),LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm= SMOTE(random_state=2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
    "# X_val_res, y_val_res = sm.fit_resample(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modeling_function(model, X_train, y_train, X_val, y_val):\n",
    "    \n",
    "    \n",
    "    # fit model on training data\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # make predictions on training and validation data\n",
    "    train_preds = model.predict(X_train)\n",
    "    val_preds = model.predict(X_val)\n",
    "    \n",
    "    # Print accuracy score\n",
    "    print('Training accuracy: ', accuracy_score(y_train, train_preds))\n",
    "    print('Validation accuracy: ', accuracy_score(y_val, val_preds))\n",
    "\n",
    "    # return fitted model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(random_state=2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy:  0.3102488384716129\n",
      "Validation accuracy:  0.2\n"
     ]
    }
   ],
   "source": [
    "dt = modeling_function(dt, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf=RandomForestClassifier(random_state=2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-8107d03ed369>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodeling_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-48-0b9939cf8440>\u001b[0m in \u001b[0;36mmodeling_function\u001b[1;34m(model, X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m# fit model on training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# make predictions on training and validation data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    384\u001b[0m             \u001b[1;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m             \u001b[1;31m# since correctness does not rely on using threads.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 386\u001b[1;33m             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n\u001b[0m\u001b[0;32m    387\u001b[0m                              \u001b[1;33m**\u001b[0m\u001b[0m_joblib_parallel_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'threads'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m                 delayed(_parallel_build_trees)(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1049\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    864\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    865\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 866\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    867\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    782\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    783\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 784\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    785\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    166\u001b[0m                                                         indices=indices)\n\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    888\u001b[0m         \"\"\"\n\u001b[0;32m    889\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 890\u001b[1;33m         super().fit(\n\u001b[0m\u001b[0;32m    891\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    892\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    373\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    374\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 375\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rf = modeling_function(rf, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg=LogisticRegression(max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy:  0.29120624660754035\n",
      "Validation accuracy:  0.3\n"
     ]
    }
   ],
   "source": [
    "logreg = modeling_function(logreg, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=modeling_function(clf, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg2 = LogisticRegression(solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy:  0.2908576635974763\n",
      "Validation accuracy:  0.3\n"
     ]
    }
   ],
   "source": [
    "logreg2 = modeling_function(logreg2, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
