{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\">\n",
    "    <ul class=\"toc-item\">\n",
    "    <li><span><a href=\"#Cleaning the Data\" data-toc-modified-id=\"Cleaning the Data\">\n",
    "        <span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Cleaning the Data</a></span>    \n",
    "    <ul class=\"toc-item\">\n",
    "        <li><span><a href=\"#Importing the Data\" data-toc-modified-id=\"Importing the Data-1.1\">\n",
    "            <span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Importing the Data</a></span></li></ul></li>\n",
    "        <li><span><a href=\"#Inspecting the data\" data-toc-modified-id=\"Inspecting the data\">\n",
    "            <span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Inspecting the data</a></span>        \n",
    "    <ul class=\"toc-item\">\n",
    "        <li><span><a href=\"#Activation-functions-(from-before)\" data-toc-modified-id=\"Activation-functions-(from-before)-2.1\">\n",
    "              <span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Activation functions (from before)</a></span>  \n",
    "    <ul class=\"toc-item\">\n",
    "        <li><span><a href=\"#Recall-what-an-activation-function-does\" data-toc-modified-id=\"Recall-what-an-activation-function-does-2.1.1\">\n",
    "              <span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Recall what an activation function does</a></span></li>\n",
    "        <li><span><a href=\"#Using-the-proper-activation\" data-toc-modified-id=\"Using-the-proper-activation-2.1.2\">\n",
    "              <span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Using the proper activation</a></span></li></ul></li>\n",
    "        <li><span><a href=\"#Random-Starts\" data-toc-modified-id=\"Random-Starts-2.2\">\n",
    "              <span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Random Starts</a></span></li>\n",
    "        <li><span><a href=\"#Momentum\" data-toc-modified-id=\"Momentum-2.3\">\n",
    "              <span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Momentum</a></span>           \n",
    "    <ul class=\"toc-item\">\n",
    "        <li><span><a href=\"#Solution?--Give-it-some-speed-ðŸ˜Ž\" data-toc-modified-id=\"Solution?--Give-it-some-speed-ðŸ˜Ž-2.3.1\">\n",
    "              <span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>Solution?  Give it some speed ðŸ˜Ž</a></span><ul class=\"toc-item\">\n",
    "        <li><span><a href=\"#Code-Implementation\" data-toc-modified-id=\"Code-Implementation-2.3.1.1\">\n",
    "              <span class=\"toc-item-num\">2.3.1.1&nbsp;&nbsp;</span>Code Implementation</a></span></li></ul></li></ul></li></ul></li>\n",
    "        <li><span><a href=\"#Make-it-Go-Faster-with-Optimizers!\" data-toc-modified-id=\"Make-it-Go-Faster-with-Optimizers!-3\">\n",
    "              <span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Make it Go Faster with Optimizers!</a></span>            \n",
    "    <ul class=\"toc-item\">\n",
    "        <li><span><a href=\"#Normalization-(Batch-Normalization)\" data-toc-modified-id=\"Normalization-(Batch-Normalization)-3.1\">\n",
    "            <span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Normalization (Batch Normalization)</a></span></li>\n",
    "        <li><span><a href=\"#Stochastic-Gradient-Descent\" data-toc-modified-id=\"Stochastic-Gradient-Descent-3.2\">\n",
    "            <span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Stochastic Gradient Descent</a></span><ul class=\"toc-item\">\n",
    "        <li><span><a href=\"#Steps\" data-toc-modified-id=\"Steps-3.2.1\">\n",
    "            <span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Steps</a></span></li></ul></li>\n",
    "        <li><span><a href=\"#AdaGrad-&amp;-RMSProp\" data-toc-modified-id=\"AdaGrad-&amp;-RMSProp-3.3\">\n",
    "            <span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>AdaGrad &amp; RMSProp</a></span>                        \n",
    "    <ul class=\"toc-item\">\n",
    "        <li><span><a href=\"#Code-Example\" data-toc-modified-id=\"Code-Example-3.3.1\">\n",
    "            <span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>Code Example</a></span></li></ul></li>\n",
    "        <li><span><a href=\"#Adam-(Adaptivr-Moment-Estimation)-Optimization-and-Other-Variants\" data-toc-modified-id=\"Adam-(Adaptivr-Moment-\n",
    "                          Estimation)-Optimization-and-Other-Variants-3.4\">\n",
    "            <span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Adam (Adaptivr Moment Estimation) Optimization and Other Variants</a></span></li>          <li><span><a href=\"#Learning-Rate-Decay\" data-toc-modified-id=\"Learning-Rate-Decay-3.5\">\n",
    "            <span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Learning Rate Decay</a></span><ul class=\"toc-item\">\n",
    "        <li><span><a href=\"#Solution?\" data-toc-modified-id=\"Solution?-3.5.1\">\n",
    "            <span class=\"toc-item-num\">3.5.1&nbsp;&nbsp;</span>Solution?</a></span></li>\n",
    "        <li><span><a href=\"#Code-Example\" data-toc-modified-id=\"Code-Example-3.5.2\">\n",
    "            <span class=\"toc-item-num\">3.5.2&nbsp;&nbsp;</span>Code Example</a></span></li>\n",
    "</ul>\n",
    "</li>\n",
    "        \n",
    "</ul>\n",
    "</li>\n",
    "        \n",
    "</ul>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Cleaning the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>headline</th>\n",
       "      <th>authors</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CRIME</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>Melissa Jeltsen</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
       "      <td>She left her husband. He killed their children...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>Andy McDonald</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/will-smit...</td>\n",
       "      <td>Of course it has a song.</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 57</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/hugh-gran...</td>\n",
       "      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/jim-carre...</td>\n",
       "      <td>The actor gives Dems an ass-kicking for not fi...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/julianna-...</td>\n",
       "      <td>The \"Dietland\" actress said using the bags is ...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                           headline  \\\n",
       "0          CRIME  There Were 2 Mass Shootings In Texas Last Week...   \n",
       "1  ENTERTAINMENT  Will Smith Joins Diplo And Nicky Jam For The 2...   \n",
       "2  ENTERTAINMENT    Hugh Grant Marries For The First Time At Age 57   \n",
       "3  ENTERTAINMENT  Jim Carrey Blasts 'Castrato' Adam Schiff And D...   \n",
       "4  ENTERTAINMENT  Julianna Margulies Uses Donald Trump Poop Bags...   \n",
       "\n",
       "           authors                                               link  \\\n",
       "0  Melissa Jeltsen  https://www.huffingtonpost.com/entry/texas-ama...   \n",
       "1    Andy McDonald  https://www.huffingtonpost.com/entry/will-smit...   \n",
       "2       Ron Dicker  https://www.huffingtonpost.com/entry/hugh-gran...   \n",
       "3       Ron Dicker  https://www.huffingtonpost.com/entry/jim-carre...   \n",
       "4       Ron Dicker  https://www.huffingtonpost.com/entry/julianna-...   \n",
       "\n",
       "                                   short_description       date  \n",
       "0  She left her husband. He killed their children... 2018-05-26  \n",
       "1                           Of course it has a song. 2018-05-26  \n",
       "2  The actor and his longtime girlfriend Anna Ebe... 2018-05-26  \n",
       "3  The actor gives Dems an ass-kicking for not fi... 2018-05-26  \n",
       "4  The \"Dietland\" actress said using the bags is ... 2018-05-26  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd \n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline \n",
    "plt.style.use('fivethirtyeight')\n",
    "import nltk # importing Natural Landuage Processing Toolit\n",
    "\n",
    "from nltk.corpus import stopwords #importing stop words to remove non-necissary words\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import f1_score, confusion_matrix, get_scorer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "import re \n",
    "import nltk\n",
    "import string\n",
    "\n",
    "\n",
    "raw_data = pd.read_json('../../../data/data.json', lines=True)\n",
    "\n",
    "raw_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Inspecting the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200853 entries, 0 to 200852\n",
      "Data columns (total 6 columns):\n",
      " #   Column             Non-Null Count   Dtype         \n",
      "---  ------             --------------   -----         \n",
      " 0   category           200853 non-null  object        \n",
      " 1   headline           200853 non-null  object        \n",
      " 2   authors            200853 non-null  object        \n",
      " 3   link               200853 non-null  object        \n",
      " 4   short_description  200853 non-null  object        \n",
      " 5   date               200853 non-null  datetime64[ns]\n",
      "dtypes: datetime64[ns](1), object(5)\n",
      "memory usage: 9.2+ MB\n"
     ]
    }
   ],
   "source": [
    "#inspecting the data types - Noticed that they're all objects \n",
    "raw_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category             0\n",
       "headline             0\n",
       "authors              0\n",
       "link                 0\n",
       "short_description    0\n",
       "date                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking For Null Values\n",
    "raw_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>headline</th>\n",
       "      <th>authors</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CRIME</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>Melissa Jeltsen</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
       "      <td>She left her husband. He killed their children...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>Andy McDonald</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/will-smit...</td>\n",
       "      <td>Of course it has a song.</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 57</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/hugh-gran...</td>\n",
       "      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/jim-carre...</td>\n",
       "      <td>The actor gives Dems an ass-kicking for not fi...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/julianna-...</td>\n",
       "      <td>The \"Dietland\" actress said using the bags is ...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                           headline  \\\n",
       "0          CRIME  There Were 2 Mass Shootings In Texas Last Week...   \n",
       "1  ENTERTAINMENT  Will Smith Joins Diplo And Nicky Jam For The 2...   \n",
       "2  ENTERTAINMENT    Hugh Grant Marries For The First Time At Age 57   \n",
       "3  ENTERTAINMENT  Jim Carrey Blasts 'Castrato' Adam Schiff And D...   \n",
       "4  ENTERTAINMENT  Julianna Margulies Uses Donald Trump Poop Bags...   \n",
       "\n",
       "           authors                                               link  \\\n",
       "0  Melissa Jeltsen  https://www.huffingtonpost.com/entry/texas-ama...   \n",
       "1    Andy McDonald  https://www.huffingtonpost.com/entry/will-smit...   \n",
       "2       Ron Dicker  https://www.huffingtonpost.com/entry/hugh-gran...   \n",
       "3       Ron Dicker  https://www.huffingtonpost.com/entry/jim-carre...   \n",
       "4       Ron Dicker  https://www.huffingtonpost.com/entry/julianna-...   \n",
       "\n",
       "                                   short_description       date  \n",
       "0  She left her husband. He killed their children... 2018-05-26  \n",
       "1                           Of course it has a song. 2018-05-26  \n",
       "2  The actor and his longtime girlfriend Anna Ebe... 2018-05-26  \n",
       "3  The actor gives Dems an ass-kicking for not fi... 2018-05-26  \n",
       "4  The \"Dietland\" actress said using the bags is ... 2018-05-26  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reassigning dataframe to df to uphold the original structure of the data - used for later references.\n",
    "df = raw_data.copy()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the Data for Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Punctuation and cleaning rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Short Description Feature**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         She left her husband. He killed their children...\n",
       "1                                  Of course it has a song.\n",
       "2         The actor and his longtime girlfriend Anna Ebe...\n",
       "3         The actor gives Dems an ass-kicking for not fi...\n",
       "4         The \"Dietland\" actress said using the bags is ...\n",
       "                                ...                        \n",
       "200848    Verizon Wireless and AT&T are already promotin...\n",
       "200849    Afterward, Azarenka, more effusive with the pr...\n",
       "200850    Leading up to Super Bowl XLVI, the most talked...\n",
       "200851    CORRECTION: An earlier version of this story i...\n",
       "200852    The five-time all-star center tore into his te...\n",
       "Name: short_description, Length: 200853, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.short_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         There Were 2 Mass Shootings In Texas Last Week...\n",
       "1         Will Smith Joins Diplo And Nicky Jam For The 2...\n",
       "2           Hugh Grant Marries For The First Time At Age 57\n",
       "3         Jim Carrey Blasts 'Castrato' Adam Schiff And D...\n",
       "4         Julianna Margulies Uses Donald Trump Poop Bags...\n",
       "                                ...                        \n",
       "200848    RIM CEO Thorsten Heins' 'Significant' Plans Fo...\n",
       "200849    Maria Sharapova Stunned By Victoria Azarenka I...\n",
       "200850    Giants Over Patriots, Jets Over Colts Among  M...\n",
       "200851    Aldon Smith Arrested: 49ers Linebacker Busted ...\n",
       "200852    Dwight Howard Rips Teammates After Magic Loss ...\n",
       "Name: headline, Length: 200853, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.headline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing Casing and punctuaton**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    she left her husband he killed their children ...\n",
      "1                              of course it has a song\n",
      "2    the actor and his longtime girlfriend anna ebe...\n",
      "3    the actor gives dems an asskicking for not fig...\n",
      "4    the dietland actress said using the bags is a ...\n",
      "Name: short_description, dtype: object\n",
      "\n",
      "============================================================\n",
      "\n",
      " 0    there were 2 mass shootings in texas last week...\n",
      "1    will smith joins diplo and nicky jam for the 2...\n",
      "2      hugh grant marries for the first time at age 57\n",
      "3    jim carrey blasts castrato adam schiff and dem...\n",
      "4    julianna margulies uses donald trump poop bags...\n",
      "Name: headline, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Removing punctuation. It helps us reduce the size of the data \n",
    "df['short_description'] = df['short_description'].str.lower().str.replace('[^\\w\\s]','')\n",
    "print(df['short_description'].head(5))\n",
    "\n",
    "print('\\n============================================================')\n",
    "\n",
    "df['headline'] = df['headline'].str.lower().str.replace('[^\\w\\s]','')\n",
    "print('\\n', df['headline'].head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merging Description and Headline together**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* checking lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.short_description[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.headline[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#merging\n",
    "df['head_description'] = df.headline + ' ' + df.short_description\n",
    "len(df.head_description[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Removing Non-Contributional Words from the Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/chandler_oneal/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#downloading stopwords to use\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#inspecting stop words \n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>head_description</th>\n",
       "      <th>stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>there were 2 mass shootings in texas last week...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>will smith joins diplo and nicky jam for the 2...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hugh grant marries for the first time at age 5...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jim carrey blasts castrato adam schiff and dem...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>julianna margulies uses donald trump poop bags...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    head_description  stopwords\n",
       "0  there were 2 mass shootings in texas last week...         12\n",
       "1  will smith joins diplo and nicky jam for the 2...          8\n",
       "2  hugh grant marries for the first time at age 5...          9\n",
       "3  jim carrey blasts castrato adam schiff and dem...          7\n",
       "4  julianna margulies uses donald trump poop bags...          8"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how many stop words do we have? \n",
    "df['stopwords'] = df['head_description'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "df[['head_description','stopwords']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>headline</th>\n",
       "      <th>authors</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "      <th>date</th>\n",
       "      <th>head_description</th>\n",
       "      <th>stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CRIME</td>\n",
       "      <td>there were 2 mass shootings in texas last week...</td>\n",
       "      <td>Melissa Jeltsen</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
       "      <td>she left her husband he killed their children ...</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>there were 2 mass shootings in texas last week...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>will smith joins diplo and nicky jam for the 2...</td>\n",
       "      <td>Andy McDonald</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/will-smit...</td>\n",
       "      <td>of course it has a song</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>will smith joins diplo and nicky jam for the 2...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                           headline  \\\n",
       "0          CRIME  there were 2 mass shootings in texas last week...   \n",
       "1  ENTERTAINMENT  will smith joins diplo and nicky jam for the 2...   \n",
       "\n",
       "           authors                                               link  \\\n",
       "0  Melissa Jeltsen  https://www.huffingtonpost.com/entry/texas-ama...   \n",
       "1    Andy McDonald  https://www.huffingtonpost.com/entry/will-smit...   \n",
       "\n",
       "                                   short_description       date  \\\n",
       "0  she left her husband he killed their children ... 2018-05-26   \n",
       "1                            of course it has a song 2018-05-26   \n",
       "\n",
       "                                    head_description  stopwords  \n",
       "0  there were 2 mass shootings in texas last week...         12  \n",
       "1  will smith joins diplo and nicky jam for the 2...          8  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspecting stop words column to identify the number of unnecissary words within each column\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing Stop Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing stopwords \n",
    "df['clean_head_description'] = df['head_description'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2 mass shootings texas last week 1 tv left hus...\n",
       "1    smith joins diplo nicky jam 2018 world cups of...\n",
       "2    hugh grant marries first time age 57 actor lon...\n",
       "3    jim carrey blasts castrato adam schiff democra...\n",
       "4    julianna margulies uses donald trump poop bags...\n",
       "Name: clean_head_description, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_head_description'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing the Text to Up Sentences Into a List of Words:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **splitting a phrase, sentence, paragraph, or an entire text document into smaller units, such as individual words or terms. Each of these smaller units are called tokens.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breaking Up Words According to Parts of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parts_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmitizing the data to reduce repitition by removing the affixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiating a lemmatizer\n",
    "lemma = WordNetLemmatizer() #instantiate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Tokenizing Data to Break Each Sentence Into Individual Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing affixes from all words\n",
    "def lemm_words(data):\n",
    "    tokens = word_tokenize(data)   #Tokenizing data \n",
    "    lemmed_tokens = [lemma.lemmatize(word) for word in tokens]   #lemmatizing data\n",
    "    return ' '.join(lemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmed = df.head_description.apply(lemm_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resetting lemmed to l to avoid overwriting issues - Do not want to re-lemmatize data(for times sake)\n",
    "l = lemmed                              #there were 2 mass shooting in texas last week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         there were 2 mass shooting in texas last week ...\n",
       "1         will smith join diplo and nicky jam for the 20...\n",
       "2         hugh grant marries for the first time at age 5...\n",
       "3         jim carrey blast castrato adam schiff and demo...\n",
       "4         julianna margulies us donald trump poop bag to...\n",
       "                                ...                        \n",
       "200848    rim ceo thorsten heins significant plan for bl...\n",
       "200849    maria sharapova stunned by victoria azarenka i...\n",
       "200850    giant over patriot jet over colt among most im...\n",
       "200851    aldon smith arrested 49ers linebacker busted f...\n",
       "200852    dwight howard rip teammate after magic loss to...\n",
       "Name: head_description, Length: 200853, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing series into a list of lists \n",
    "df['unique_word'] = l.str.split(' ')   # [there, were, 2, mass, shooting, in, texas, la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[there, were, 2, mass, shooting, in, texas, la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[will, smith, join, diplo, and, nicky, jam, fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[hugh, grant, marries, for, the, first, time, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[jim, carrey, blast, castrato, adam, schiff, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[julianna, margulies, us, donald, trump, poop,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200848</th>\n",
       "      <td>[rim, ceo, thorsten, heins, significant, plan,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200849</th>\n",
       "      <td>[maria, sharapova, stunned, by, victoria, azar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200850</th>\n",
       "      <td>[giant, over, patriot, jet, over, colt, among,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200851</th>\n",
       "      <td>[aldon, smith, arrested, 49ers, linebacker, bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200852</th>\n",
       "      <td>[dwight, howard, rip, teammate, after, magic, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200853 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              unique_word\n",
       "0       [there, were, 2, mass, shooting, in, texas, la...\n",
       "1       [will, smith, join, diplo, and, nicky, jam, fo...\n",
       "2       [hugh, grant, marries, for, the, first, time, ...\n",
       "3       [jim, carrey, blast, castrato, adam, schiff, a...\n",
       "4       [julianna, margulies, us, donald, trump, poop,...\n",
       "...                                                   ...\n",
       "200848  [rim, ceo, thorsten, heins, significant, plan,...\n",
       "200849  [maria, sharapova, stunned, by, victoria, azar...\n",
       "200850  [giant, over, patriot, jet, over, colt, among,...\n",
       "200851  [aldon, smith, arrested, 49ers, linebacker, bu...\n",
       "200852  [dwight, howard, rip, teammate, after, magic, ...\n",
       "\n",
       "[200853 rows x 1 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['unique_word']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Creating a Bag of Words for Vectorization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list containing all words \n",
    "\n",
    "unique_lemmed_list = []                #the  (appears ->)  261830\n",
    "for x in df['unique_word']:  \n",
    "    unique_lemmed_list += x  # append elements of lists to full list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Removing Single Characters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5855301"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#inspecting the original number of words\n",
    "len(unique_lemmed_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looping through and adding words > len(1) to a new list \n",
    "full_words = [] # creating quotes \n",
    "index = 0\n",
    "for words in unique_lemmed_list: \n",
    "    if len(words) > 1:\n",
    "        full_words.append(words)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5621546"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking the NEW number of words after removal\n",
    "len(full_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Changing New Bag of Words into a Series for TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "the          261830\n",
       "to           163957\n",
       "of           128119\n",
       "and          119623\n",
       "in            95541\n",
       "              ...  \n",
       "menÃ©ndez          1\n",
       "telfer            1\n",
       "idk               1\n",
       "upcharges         1\n",
       "peopleit          1\n",
       "Length: 102078, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_value_counts = pd.Series(full_words).value_counts()\n",
    "word_value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>261830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>to</td>\n",
       "      <td>163957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>of</td>\n",
       "      <td>128119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>and</td>\n",
       "      <td>119623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>in</td>\n",
       "      <td>95541</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  word   count\n",
       "0  the  261830\n",
       "1   to  163957\n",
       "2   of  128119\n",
       "3  and  119623\n",
       "4   in   95541"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot = pd.DataFrame(data = word_value_counts).reset_index()\n",
    "plot.columns = ['word','count']\n",
    "plot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to do --- calculating sentament of text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.short_description[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['len_description_head'] = [len(df.short_description[x]) for x in range(len(df))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Merging Similar Columns:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>headline</th>\n",
       "      <th>authors</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "      <th>date</th>\n",
       "      <th>head_description</th>\n",
       "      <th>stopwords</th>\n",
       "      <th>clean_head_description</th>\n",
       "      <th>unique_word</th>\n",
       "      <th>len_description_head</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GENERAL</td>\n",
       "      <td>there were 2 mass shootings in texas last week...</td>\n",
       "      <td>Melissa Jeltsen</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
       "      <td>she left her husband he killed their children ...</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>there were 2 mass shootings in texas last week...</td>\n",
       "      <td>12</td>\n",
       "      <td>2 mass shootings texas last week 1 tv left hus...</td>\n",
       "      <td>[there, were, 2, mass, shooting, in, texas, la...</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  category                                           headline  \\\n",
       "0  GENERAL  there were 2 mass shootings in texas last week...   \n",
       "\n",
       "           authors                                               link  \\\n",
       "0  Melissa Jeltsen  https://www.huffingtonpost.com/entry/texas-ama...   \n",
       "\n",
       "                                   short_description       date  \\\n",
       "0  she left her husband he killed their children ... 2018-05-26   \n",
       "\n",
       "                                    head_description  stopwords  \\\n",
       "0  there were 2 mass shootings in texas last week...         12   \n",
       "\n",
       "                              clean_head_description  \\\n",
       "0  2 mass shootings texas last week 1 tv left hus...   \n",
       "\n",
       "                                         unique_word  len_description_head  \n",
       "0  [there, were, 2, mass, shooting, in, texas, la...                    73  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Visualizing current columns**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of categories: 41\n",
      "\n",
      "\n",
      "Listing Categories: POLITICS          32739\n",
      "WELLNESS          17827\n",
      "ENTERTAINMENT     16058\n",
      "TRAVEL             9887\n",
      "STYLE & BEAUTY     9649\n",
      "PARENTING          8677\n",
      "HEALTHY LIVING     6694\n",
      "QUEER VOICES       6314\n",
      "FOOD & DRINK       6226\n",
      "BUSINESS           5937\n",
      "COMEDY             5175\n",
      "SPORTS             4884\n",
      "BLACK VOICES       4528\n",
      "HOME & LIVING      4195\n",
      "PARENTS            3955\n",
      "THE WORLDPOST      3664\n",
      "WEDDINGS           3651\n",
      "WOMEN              3490\n",
      "IMPACT             3459\n",
      "DIVORCE            3426\n",
      "CRIME              3405\n",
      "MEDIA              2815\n",
      "WEIRD NEWS         2670\n",
      "GREEN              2622\n",
      "WORLDPOST          2579\n",
      "RELIGION           2556\n",
      "STYLE              2254\n",
      "SCIENCE            2178\n",
      "WORLD NEWS         2177\n",
      "TASTE              2096\n",
      "TECH               2082\n",
      "MONEY              1707\n",
      "ARTS               1509\n",
      "FIFTY              1401\n",
      "GOOD NEWS          1398\n",
      "ARTS & CULTURE     1339\n",
      "ENVIRONMENT        1323\n",
      "COLLEGE            1144\n",
      "LATINO VOICES      1129\n",
      "CULTURE & ARTS     1030\n",
      "EDUCATION          1004\n",
      "Name: category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#total number of categories\n",
    "print('# of categories:', df.category.value_counts().nunique())\n",
    "\n",
    "print('\\n\\nListing Categories:', df.category.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Combining columns**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['POLITICS', 'WELLNESS', 'ENTERTAINMENT', 'TRAVEL', 'STYLE & BEAUTY',\n",
       "       'PARENTING', 'HEALTHY LIVING', 'QUEER VOICES', 'FOOD & DRINK',\n",
       "       'BUSINESS', 'COMEDY', 'SPORTS', 'BLACK VOICES', 'HOME & LIVING',\n",
       "       'PARENTS', 'THE WORLDPOST', 'WEDDINGS', 'WOMEN', 'IMPACT', 'DIVORCE',\n",
       "       'CRIME', 'MEDIA', 'WEIRD NEWS', 'GREEN', 'WORLDPOST', 'RELIGION',\n",
       "       'STYLE', 'SCIENCE', 'WORLD NEWS', 'TASTE', 'TECH', 'MONEY', 'ARTS',\n",
       "       'FIFTY', 'GOOD NEWS', 'ARTS & CULTURE', 'ENVIRONMENT', 'COLLEGE',\n",
       "       'LATINO VOICES', 'CULTURE & ARTS', 'EDUCATION'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = df['category'].value_counts().index\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupper(grouplist,name):\n",
    "    for idx in categories: #for each category name \n",
    "        if idx in grouplist: #if the category name appears in the grouplist below \n",
    "            df.loc[df['category'] == idx, 'category'] = name  #assign it to name = \" \"\n",
    "\n",
    "\n",
    "\n",
    "groupper( grouplist= ['WELLNESS', 'HEALTHY LIVING','HOME & LIVING','STYLE & BEAUTY' ,'STYLE'] , name =  'LIFESTYLE AND WELLNESS')\n",
    "groupper( grouplist= [ 'PARENTING', 'PARENTS' ,'EDUCATION' ,'COLLEGE'] , name =  'PARENTING AND EDUCATION')\n",
    "groupper( grouplist= ['SPORTS','ENTERTAINMENT' , 'COMEDY','WEIRD NEWS','ARTS'] , name =  'SPORTS AND ENTERTAINMENT')\n",
    "groupper( grouplist= ['TRAVEL', 'ARTS & CULTURE','CULTURE & ARTS','FOOD & DRINK', 'TASTE'] , name =  'TRAVEL-TOURISM & ART-CULTURE')\n",
    "groupper( grouplist= ['WOMEN','QUEER VOICES', 'LATINO VOICES', 'BLACK VOICES'] , name =  'EMPOWERED VOICES')\n",
    "groupper( grouplist= ['BUSINESS' ,  'MONEY'] , name =  'BUSINESS-MONEY')\n",
    "groupper( grouplist= ['THE WORLDPOST' , 'WORLDPOST' , 'WORLD NEWS'] , name =  'WORLDNEWS')\n",
    "groupper( grouplist= ['ENVIRONMENT' ,'GREEN'] , name =  'ENVIRONMENT')\n",
    "groupper( grouplist= ['TECH', 'SCIENCE'] , name =  'SCIENCE AND TECH')\n",
    "groupper( grouplist= ['FIFTY' , 'IMPACT' ,'GOOD NEWS','CRIME'] , name =  'GENERAL')\n",
    "groupper( grouplist= ['WEDDINGS', 'DIVORCE',  'RELIGION','MEDIA'] , name =  'MISC')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Inspecting new columns**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of categories: 12\n",
      "\n",
      "\n",
      "Listing Categories: LIFESTYLE AND WELLNESS          40619\n",
      "POLITICS                        32739\n",
      "SPORTS AND ENTERTAINMENT        30296\n",
      "TRAVEL-TOURISM & ART-CULTURE    20578\n",
      "EMPOWERED VOICES                15461\n",
      "PARENTING AND EDUCATION         14780\n",
      "MISC                            12448\n",
      "GENERAL                          9663\n",
      "WORLDNEWS                        8420\n",
      "BUSINESS-MONEY                   7644\n",
      "SCIENCE AND TECH                 4260\n",
      "ENVIRONMENT                      3945\n",
      "Name: category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#inspecting newly combined categories\n",
    "print('# of categories:', df.category.value_counts().nunique())\n",
    "print('\\n\\nListing Categories:', df.category.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sorting Words Using Parts of Speech Sepparator Function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **TF - IDF**\n",
    "\n",
    "**Term Frequency â€” Inverse Document Frequency** - Derermining word importance to documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Vectorizing the (title description + full description) column (converting it into numeric data).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #creating a function to pass into the analyzer to clean the raw data \n",
    "    \n",
    "# def clean_history(history):\n",
    "#     history = \"\".join([word for word in history if word not in string.punctuation])\n",
    "#     tokens = re.split('\\W+', history)\n",
    "#     history = [wn.lemmatize(word) for word in tokens if word not in stopwords]\n",
    "#     return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returning Trigrams - Groups of Three Words\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf = tfidf_vectorizer.fit_transform(df['head_description'])\n",
    "features = (tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10', '100', '11', '12', '13', '14', '15', '20', '2012', '2013']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Shape of tfidf : \n",
      " (200853, 1200)\n",
      "\n",
      "\n",
      "Features : \n",
      " ['10', '100', '11', '12', '13', '14', '15', '20', '2012', '2013', '2014', '2015', '2016', '25', '30', '50', 'able', 'about', 'above', 'abuse', 'access', 'according', 'accused', 'across', 'act', 'action', 'actor', 'actress', 'actually', 'ad', 'address', 'administration', 'adorable', 'adults', 'advice', 'after', 'again', 'against', 'age', 'ago', 'ahead', 'air', 'all', 'allegedly', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'amazing', 'america', 'american', 'americans', 'americas', 'among', 'an', 'and', 'animal', 'anniversary', 'announced', 'another', 'answer', 'any', 'anyone', 'anything', 'apple', 'april', 'are', 'arent', 'around', 'arrested', 'art', 'artist', 'as', 'ask', 'asked', 'assault', 'at', 'attack', 'attacks', 'attention', 'attorney', 'avoid', 'awards', 'awareness', 'away', 'awesome', 'babies', 'baby', 'back', 'bad', 'ban', 'bank', 'based', 'battle', 'be', 'beach', 'beautiful', 'beauty', 'became', 'because', 'become', 'becoming', 'been', 'before', 'begin', 'behavior', 'behind', 'being', 'believe', 'below', 'benefits', 'bernie', 'best', 'better', 'between', 'beyond', 'big', 'biggest', 'bill', 'billion', 'birth', 'birthday', 'bit', 'black', 'blood', 'blue', 'body', 'book', 'books', 'born', 'both', 'bowl', 'box', 'boy', 'boys', 'brain', 'break', 'bring', 'british', 'brought', 'brown', 'budget', 'build', 'building', 'business', 'but', 'buy', 'by', 'cake', 'california', 'call', 'called', 'calling', 'calls', 'came', 'campaign', 'can', 'cancer', 'candidate', 'candidates', 'cannot', 'cant', 'car', 'care', 'career', 'case', 'caught', 'cause', 'celebrate', 'celebrities', 'celebrity', 'center', 'ceo', 'certain', 'certainly', 'challenge', 'challenges', 'chance', 'change', 'changed', 'changes', 'changing', 'check', 'chief', 'child', 'children', 'china', 'chocolate', 'choice', 'choose', 'chris', 'christmas', 'church', 'cities', 'city', 'civil', 'claims', 'class', 'classic', 'clean', 'clear', 'click', 'climate', 'clinton', 'close', 'coffee', 'cold', 'college', 'color', 'come', 'comes', 'coming', 'comments', 'common', 'community', 'companies', 'company', 'completely', 'congress', 'consider', 'continue', 'continues', 'control', 'conversation', 'cool', 'cost', 'could', 'couldnt', 'countries', 'country', 'couple', 'couples', 'course', 'court', 'cover', 'crazy', 'create', 'created', 'creating', 'creative', 'credit', 'crisis', 'cruz', 'culture', 'current', 'cut', 'dad', 'dads', 'daily', 'dangerous', 'dark', 'data', 'date', 'dating', 'daughter', 'david', 'day', 'days', 'dead', 'deal', 'dear', 'death', 'debate', 'decades', 'decided', 'decision', 'deep', 'democratic', 'democrats', 'department', 'designer', 'despite', 'did', 'didnt', 'died', 'diet', 'difference', 'different', 'difficult', 'dinner', 'director', 'disease', 'divorce', 'do', 'doctors', 'does', 'doesnt', 'dog', 'dogs', 'doing', 'donald', 'done', 'dont', 'down', 'dr', 'dream', 'dreams', 'dress', 'drink', 'drug', 'during', 'each', 'early', 'earth', 'easier', 'east', 'easy', 'eat', 'eating', 'economic', 'economy', 'education', 'effort', 'either', 'election', 'else', 'email', 'emotional', 'end', 'energy', 'enjoy', 'enough', 'entire', 'especially', 'even', 'event', 'events', 'ever', 'every', 'everyone', 'everything', 'evidence', 'exactly', 'executive', 'exercise', 'expect', 'experience', 'experts', 'eyes', 'face', 'facebook', 'faces', 'fact', 'fall', 'families', 'family', 'famous', 'fan', 'fans', 'far', 'fashion', 'fast', 'father', 'fathers', 'favorite', 'fear', 'federal', 'feel', 'feeling', 'feels', 'felt', 'female', 'festival', 'few', 'fight', 'fighting', 'film', 'final', 'finally', 'financial', 'find', 'finding', 'finds', 'fire', 'first', 'fit', 'five', 'flight', 'florida', 'focus', 'follow', 'following', 'food', 'foods', 'football', 'for', 'force', 'foreign', 'forget', 'form', 'former', 'forward', 'found', 'four', 'fox', 'free', 'freedom', 'french', 'fresh', 'friday', 'friend', 'friends', 'from', 'front', 'full', 'fun', 'future', 'game', 'games', 'gave', 'gay', 'gender', 'general', 'george', 'get', 'gets', 'getting', 'gift', 'gifts', 'girl', 'girls', 'give', 'given', 'gives', 'giving', 'global', 'go', 'god', 'goes', 'going', 'gold', 'golden', 'gone', 'good', 'gop', 'got', 'government', 'governor', 'great', 'greatest', 'green', 'group', 'groups', 'grow', 'growing', 'guide', 'gun', 'guy', 'had', 'hair', 'half', 'halloween', 'hand', 'hands', 'happen', 'happened', 'happens', 'happiness', 'happy', 'hard', 'has', 'hate', 'have', 'having', 'he', 'head', 'health', 'healthy', 'hear', 'heard', 'heart', 'hell', 'help', 'helped', 'helping', 'her', 'here', 'heres', 'hes', 'high', 'higher', 'hill', 'hillary', 'him', 'himself', 'his', 'history', 'hit', 'hold', 'holiday', 'holidays', 'hollywood', 'home', 'honor', 'hope', 'hospital', 'host', 'hot', 'hotel', 'hotels', 'hours', 'house', 'how', 'however', 'huffpost', 'huge', 'human', 'hundreds', 'husband', 'ice', 'id', 'idea', 'ideas', 'if', 'ill', 'im', 'imagine', 'immigration', 'impact', 'important', 'in', 'including', 'industry', 'information', 'inside', 'inspired', 'instagram', 'instead', 'international', 'internet', 'interview', 'into', 'investigation', 'iran', 'is', 'isis', 'island', 'isnt', 'issue', 'issues', 'it', 'its', 'itself', 'ive', 'james', 'jennifer', 'jimmy', 'job', 'jobs', 'joe', 'john', 'journey', 'joy', 'judge', 'july', 'just', 'justice', 'kardashian', 'kate', 'keep', 'keeping', 'key', 'kid', 'kids', 'kill', 'killed', 'killing', 'kim', 'kind', 'king', 'kitchen', 'knew', 'know', 'known', 'knows', 'korea', 'la', 'labor', 'lady', 'last', 'late', 'later', 'latest', 'law', 'lead', 'leader', 'leaders', 'learn', 'learned', 'learning', 'least', 'leave', 'left', 'legal', 'less', 'lessons', 'let', 'lets', 'letter', 'level', 'lgbt', 'life', 'light', 'like', 'likely', 'line', 'linked', 'list', 'listen', 'little', 'live', 'lives', 'living', 'local', 'london', 'long', 'longer', 'look', 'looking', 'looks', 'lose', 'losing', 'loss', 'lost', 'lot', 'love', 'loved', 'made', 'magazine', 'major', 'make', 'makes', 'makeup', 'making', 'man', 'many', 'march', 'mark', 'market', 'marriage', 'married', 'matter', 'may', 'maybe', 'me', 'mean', 'means', 'media', 'medical', 'meditation', 'meet', 'meeting', 'members', 'men', 'mental', 'message', 'met', 'michael', 'michelle', 'middle', 'might', 'mike', 'military', 'million', 'millions', 'mind', 'minutes', 'miss', 'missing', 'model', 'modern', 'mom', 'moment', 'moments', 'moms', 'monday', 'money', 'month', 'months', 'more', 'morning', 'most', 'mother', 'mothers', 'move', 'movement', 'movie', 'moving', 'much', 'music', 'muslim', 'must', 'my', 'myself', 'name', 'nation', 'national', 'nations', 'natural', 'nature', 'near', 'nearly', 'need', 'needed', 'needs', 'network', 'never', 'new', 'news', 'next', 'nfl', 'night', 'no', 'nominee', 'north', 'not', 'nothing', 'now', 'number', 'obama', 'obamacare', 'obamas', 'of', 'off', 'offer', 'offers', 'office', 'official', 'officials', 'often', 'oil', 'old', 'olympic', 'on', 'once', 'one', 'ones', 'online', 'only', 'open', 'opportunity', 'or', 'order', 'other', 'others', 'our', 'ourselves', 'out', 'outside', 'over', 'own', 'pain', 'parent', 'parenting', 'parents', 'paris', 'park', 'part', 'party', 'past', 'patients', 'paul', 'pay', 'peace', 'people', 'percent', 'perfect', 'performance', 'perhaps', 'person', 'personal', 'phone', 'photo', 'photos', 'pick', 'picture', 'pinterest', 'place', 'places', 'plan', 'planned', 'planning', 'plans', 'play', 'playing', 'please', 'point', 'police', 'policy', 'political', 'politics', 'poll', 'poor', 'popular', 'positive', 'possible', 'post', 'potential', 'pounds', 'power', 'powerful', 'practice', 'pregnant', 'present', 'president', 'presidential', 'presidents', 'press', 'pretty', 'primary', 'prison', 'private', 'probably', 'problem', 'problems', 'process', 'products', 'program', 'project', 'protect', 'public', 'put', 'queer', 'question', 'questions', 'quite', 'race', 'raise', 'rather', 'read', 'ready', 'real', 'reality', 'really', 'reason', 'reasons', 'recent', 'recently', 'recipe', 'recipes', 'record', 'red', 'relationship', 'relationships', 'release', 'released', 'religious', 'remember', 'report', 'reportedly', 'reports', 'republican', 'republicans', 'research', 'researchers', 'response', 'rest', 'results', 'return', 'reveals', 'right', 'rights', 'rise', 'risk', 'road', 'rock', 'role', 'room', 'rules', 'run', 'running', 'russia', 'russian', 'ryan', 'safe', 'safety', 'said', 'same', 'san', 'sanders', 'saturday', 'save', 'saw', 'say', 'saying', 'says', 'school', 'schools', 'science', 'scientists', 'scott', 'season', 'second', 'secret', 'secretary', 'security', 'see', 'seem', 'seems', 'seen', 'senate', 'senator', 'sense', 'series', 'serious', 'service', 'set', 'seven', 'several', 'sex', 'sexual', 'share', 'shares', 'she', 'shes', 'shooting', 'shopping', 'short', 'shot', 'should', 'shouldnt', 'show', 'shows', 'side', 'sign', 'signs', 'simple', 'simply', 'since', 'singer', 'single', 'six', 'skin', 'sleep', 'small', 'snl', 'so', 'social', 'society', 'some', 'someone', 'something', 'sometimes', 'son', 'song', 'soon', 'soul', 'south', 'space', 'speak', 'special', 'speech', 'spend', 'spending', 'spent', 'sports', 'spring', 'st', 'stage', 'stand', 'star', 'stars', 'start', 'started', 'state', 'states', 'stay', 'step', 'stephen', 'steps', 'still', 'stop', 'store', 'stories', 'story', 'street', 'stress', 'strong', 'student', 'students', 'study', 'style', 'success', 'successful', 'such', 'suggests', 'summer', 'sunday', 'super', 'support', 'supreme', 'sure', 'surprise', 'sweet', 'system', 'table', 'take', 'taken', 'takes', 'taking', 'talk', 'talking', 'talks', 'tax', 'taylor', 'teach', 'team', 'technology', 'ted', 'teen', 'teens', 'tell', 'tells', 'test', 'texas', 'than', 'thank', 'thanks', 'thanksgiving', 'that', 'thats', 'the', 'their', 'them', 'themselves', 'then', 'there', 'theres', 'these', 'they', 'theyre', 'thing', 'things', 'think', 'thinking', 'thinks', 'third', 'this', 'those', 'though', 'thought', 'thoughts', 'thousands', 'three', 'through', 'time', 'times', 'tips', 'to', 'today', 'together', 'told', 'tom', 'too', 'took', 'top', 'totally', 'touch', 'tour', 'toward', 'town', 'trans', 'transgender', 'travel', 'treatment', 'tried', 'trip', 'true', 'truly', 'trump', 'trumps', 'truth', 'try', 'trying', 'tuesday', 'tumblr', 'turn', 'turned', 'turns', 'tv', 'tweets', 'twitter', 'two', 'under', 'understand', 'union', 'unique', 'united', 'university', 'until', 'up', 'upon', 'us', 'use', 'used', 'using', 'vacation', 'valentines', 'very', 'victims', 'video', 'view', 'violence', 'visit', 'voice', 'vote', 'voters', 'vs', 'wait', 'waiting', 'walk', 'wall', 'want', 'wanted', 'wants', 'war', 'wars', 'was', 'washington', 'wasnt', 'watch', 'watching', 'water', 'way', 'ways', 'we', 'wear', 'wearing', 'weather', 'wedding', 'weddings', 'week', 'weekend', 'weeks', 'weight', 'welcome', 'well', 'went', 'were', 'west', 'weve', 'what', 'whats', 'when', 'where', 'whether', 'which', 'while', 'white', 'who', 'whole', 'whose', 'why', 'wife', 'will', 'win', 'wine', 'wins', 'winter', 'wish', 'with', 'within', 'without', 'woman', 'women', 'womens', 'wonder', 'wont', 'word', 'words', 'work', 'workers', 'working', 'works', 'world', 'worlds', 'worse', 'worst', 'worth', 'would', 'wouldnt', 'wrong', 'wrote', 'year', 'years', 'yes', 'yet', 'yoga', 'york', 'you', 'youll', 'young', 'your', 'youre', 'yourself', 'youve']\n",
      "\n",
      "\n",
      "X1 : \n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\nShape of tfidf : \\n\", X_tfidf.shape)\n",
    "\n",
    "\n",
    "#printing out the features \n",
    "print(\"\\n\\nFeatures : \\n\", features)\n",
    "print(\"\\n\\nX1 : \\n\", X_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=features)\n",
    "# X_tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>20</th>\n",
       "      <th>2012</th>\n",
       "      <th>2013</th>\n",
       "      <th>...</th>\n",
       "      <th>yet</th>\n",
       "      <th>yoga</th>\n",
       "      <th>york</th>\n",
       "      <th>you</th>\n",
       "      <th>youll</th>\n",
       "      <th>young</th>\n",
       "      <th>your</th>\n",
       "      <th>youre</th>\n",
       "      <th>yourself</th>\n",
       "      <th>youve</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200848</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200849</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200850</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200851</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200852</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200853 rows Ã— 1200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         10  100   11   12   13   14   15   20  2012  2013  ...  yet  yoga  \\\n",
       "0       0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0   0.0  ...  0.0   0.0   \n",
       "1       0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0   0.0  ...  0.0   0.0   \n",
       "2       0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0   0.0  ...  0.0   0.0   \n",
       "3       0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0   0.0  ...  0.0   0.0   \n",
       "4       0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0   0.0  ...  0.0   0.0   \n",
       "...     ...  ...  ...  ...  ...  ...  ...  ...   ...   ...  ...  ...   ...   \n",
       "200848  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0   0.0  ...  0.0   0.0   \n",
       "200849  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0   0.0  ...  0.0   0.0   \n",
       "200850  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0   0.0  ...  0.0   0.0   \n",
       "200851  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0   0.0  ...  0.0   0.0   \n",
       "200852  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0   0.0  ...  0.0   0.0   \n",
       "\n",
       "        york  you  youll  young  your  youre  yourself  youve  \n",
       "0        0.0  0.0    0.0    0.0   0.0    0.0       0.0    0.0  \n",
       "1        0.0  0.0    0.0    0.0   0.0    0.0       0.0    0.0  \n",
       "2        0.0  0.0    0.0    0.0   0.0    0.0       0.0    0.0  \n",
       "3        0.0  0.0    0.0    0.0   0.0    0.0       0.0    0.0  \n",
       "4        0.0  0.0    0.0    0.0   0.0    0.0       0.0    0.0  \n",
       "...      ...  ...    ...    ...   ...    ...       ...    ...  \n",
       "200848   0.0  0.0    0.0    0.0   0.0    0.0       0.0    0.0  \n",
       "200849   0.0  0.0    0.0    0.0   0.0    0.0       0.0    0.0  \n",
       "200850   0.0  0.0    0.0    0.0   0.0    0.0       0.0    0.0  \n",
       "200851   0.0  0.0    0.0    0.0   0.0    0.0       0.0    0.0  \n",
       "200852   0.0  0.0    0.0    0.0   0.0    0.0       0.0    0.0  \n",
       "\n",
       "[200853 rows x 1200 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#inspecting values for imbalances to determine whether or not to SMOTE the data in the upcoming pipeline\n",
    "X_tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Vectorized Data with Description Length and Stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_desc_stop_df = pd.concat([X_tfidf_df, df.len_description_head, df.stopwords], axis=1, join='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>20</th>\n",
       "      <th>2012</th>\n",
       "      <th>2013</th>\n",
       "      <th>...</th>\n",
       "      <th>york</th>\n",
       "      <th>you</th>\n",
       "      <th>youll</th>\n",
       "      <th>young</th>\n",
       "      <th>your</th>\n",
       "      <th>youre</th>\n",
       "      <th>yourself</th>\n",
       "      <th>youve</th>\n",
       "      <th>len_description_head</th>\n",
       "      <th>stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>81</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200848</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200849</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>117</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200850</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>119</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200851</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>119</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200852</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>120</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200853 rows Ã— 1202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         10  100   11   12   13   14   15   20  2012  2013  ...  york  you  \\\n",
       "0       0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0   0.0  ...   0.0  0.0   \n",
       "1       0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0   0.0  ...   0.0  0.0   \n",
       "2       0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0   0.0  ...   0.0  0.0   \n",
       "3       0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0   0.0  ...   0.0  0.0   \n",
       "4       0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0   0.0  ...   0.0  0.0   \n",
       "...     ...  ...  ...  ...  ...  ...  ...  ...   ...   ...  ...   ...  ...   \n",
       "200848  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0   0.0  ...   0.0  0.0   \n",
       "200849  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0   0.0  ...   0.0  0.0   \n",
       "200850  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0   0.0  ...   0.0  0.0   \n",
       "200851  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0   0.0  ...   0.0  0.0   \n",
       "200852  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0   0.0  ...   0.0  0.0   \n",
       "\n",
       "        youll  young  your  youre  yourself  youve  len_description_head  \\\n",
       "0         0.0    0.0   0.0    0.0       0.0    0.0                    73   \n",
       "1         0.0    0.0   0.0    0.0       0.0    0.0                    23   \n",
       "2         0.0    0.0   0.0    0.0       0.0    0.0                    86   \n",
       "3         0.0    0.0   0.0    0.0       0.0    0.0                    84   \n",
       "4         0.0    0.0   0.0    0.0       0.0    0.0                    81   \n",
       "...       ...    ...   ...    ...       ...    ...                   ...   \n",
       "200848    0.0    0.0   0.0    0.0       0.0    0.0                   120   \n",
       "200849    0.0    0.0   0.0    0.0       0.0    0.0                   117   \n",
       "200850    0.0    0.0   0.0    0.0       0.0    0.0                   119   \n",
       "200851    0.0    0.0   0.0    0.0       0.0    0.0                   119   \n",
       "200852    0.0    0.0   0.0    0.0       0.0    0.0                   120   \n",
       "\n",
       "        stopwords  \n",
       "0              12  \n",
       "1               8  \n",
       "2               9  \n",
       "3               7  \n",
       "4               8  \n",
       "...           ...  \n",
       "200848          5  \n",
       "200849         10  \n",
       "200850         17  \n",
       "200851          9  \n",
       "200852          7  \n",
       "\n",
       "[200853 rows x 1202 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf_desc_stop_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**inspecting vectorized rows that are greater than 0 and returning them**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>20</th>\n",
       "      <th>2012</th>\n",
       "      <th>2013</th>\n",
       "      <th>...</th>\n",
       "      <th>york</th>\n",
       "      <th>you</th>\n",
       "      <th>youll</th>\n",
       "      <th>young</th>\n",
       "      <th>your</th>\n",
       "      <th>youre</th>\n",
       "      <th>yourself</th>\n",
       "      <th>youve</th>\n",
       "      <th>len_description_head</th>\n",
       "      <th>stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>0.234586</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.137365</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>0.227994</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>0.244635</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.263495</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>121</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>0.329968</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818</th>\n",
       "      <td>0.283676</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.365122</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200658</th>\n",
       "      <td>0.249799</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>117</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200667</th>\n",
       "      <td>0.194012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>137</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200687</th>\n",
       "      <td>0.223837</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>170</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200757</th>\n",
       "      <td>0.156474</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>193</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200794</th>\n",
       "      <td>0.235027</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>119</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3954 rows Ã— 1202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              10  100        11   12   13   14   15   20  2012  2013  ...  \\\n",
       "305     0.234586  0.0  0.000000  0.0  0.0  0.0  0.0  0.0   0.0   0.0  ...   \n",
       "360     0.227994  0.0  0.000000  0.0  0.0  0.0  0.0  0.0   0.0   0.0  ...   \n",
       "517     0.244635  0.0  0.000000  0.0  0.0  0.0  0.0  0.0   0.0   0.0  ...   \n",
       "781     0.329968  0.0  0.000000  0.0  0.0  0.0  0.0  0.0   0.0   0.0  ...   \n",
       "818     0.283676  0.0  0.365122  0.0  0.0  0.0  0.0  0.0   0.0   0.0  ...   \n",
       "...          ...  ...       ...  ...  ...  ...  ...  ...   ...   ...  ...   \n",
       "200658  0.249799  0.0  0.000000  0.0  0.0  0.0  0.0  0.0   0.0   0.0  ...   \n",
       "200667  0.194012  0.0  0.000000  0.0  0.0  0.0  0.0  0.0   0.0   0.0  ...   \n",
       "200687  0.223837  0.0  0.000000  0.0  0.0  0.0  0.0  0.0   0.0   0.0  ...   \n",
       "200757  0.156474  0.0  0.000000  0.0  0.0  0.0  0.0  0.0   0.0   0.0  ...   \n",
       "200794  0.235027  0.0  0.000000  0.0  0.0  0.0  0.0  0.0   0.0   0.0  ...   \n",
       "\n",
       "            york       you  youll  young  your  youre  yourself  youve  \\\n",
       "305     0.000000  0.137365    0.0    0.0   0.0    0.0       0.0    0.0   \n",
       "360     0.000000  0.000000    0.0    0.0   0.0    0.0       0.0    0.0   \n",
       "517     0.263495  0.000000    0.0    0.0   0.0    0.0       0.0    0.0   \n",
       "781     0.000000  0.000000    0.0    0.0   0.0    0.0       0.0    0.0   \n",
       "818     0.000000  0.000000    0.0    0.0   0.0    0.0       0.0    0.0   \n",
       "...          ...       ...    ...    ...   ...    ...       ...    ...   \n",
       "200658  0.000000  0.000000    0.0    0.0   0.0    0.0       0.0    0.0   \n",
       "200667  0.000000  0.000000    0.0    0.0   0.0    0.0       0.0    0.0   \n",
       "200687  0.000000  0.000000    0.0    0.0   0.0    0.0       0.0    0.0   \n",
       "200757  0.000000  0.000000    0.0    0.0   0.0    0.0       0.0    0.0   \n",
       "200794  0.000000  0.000000    0.0    0.0   0.0    0.0       0.0    0.0   \n",
       "\n",
       "        len_description_head  stopwords  \n",
       "305                       78          7  \n",
       "360                       89         12  \n",
       "517                      121          8  \n",
       "781                       76          6  \n",
       "818                       48          8  \n",
       "...                      ...        ...  \n",
       "200658                   117         11  \n",
       "200667                   137         12  \n",
       "200687                   170         22  \n",
       "200757                   193         17  \n",
       "200794                   119          6  \n",
       "\n",
       "[3954 rows x 1202 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# idf_desc_stop_df[idf_desc_stop_df.iloc[:,0] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Preparatition:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating a function to shorten the modeling process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a function to pass in our model that will then return an F-1 Score\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "\n",
    "def make_train_scorer(model, X_train, y_train, X_val, y_val):\n",
    "    model.fit(X_train, y_train)\n",
    "    scorer = make_scorer(f1_score, average='weighted')\n",
    "    \n",
    "    train_f1 = scorer(model, X_train, y_train)\n",
    "    val_f1 = scorer(model, X_val, y_val)\n",
    "    \n",
    "    print('Train Score:', train_f1)\n",
    "    print('Validation Score:', val_f1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_test_scorer(model, X_test, y_test):    \n",
    "    model.fit(X_train, y_train)\n",
    "    scorer = make_scorer(f1_score, average='weighted')\n",
    "    \n",
    "    test_f1 = scorer(model, X_test, y_test)\n",
    "    print('Test Score:', test_f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Splitting the vecorized data into the variable X and the Categories into the variable y**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = idf_desc_stop_df\n",
    "y = df.category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_test, y_tr, y_test = train_test_split(X, y, random_state=3, test_size=.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_tr, y_tr, random_state=3, test_size=.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rebalancing Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=3)\n",
    "\n",
    "X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "X_val_res, y_val_res = sm.fit_resample(X_val, y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train resampled: 311580\n",
      "=========================\n"
     ]
    }
   ],
   "source": [
    "print('X_train resampled:', X_train_res.value_counts().sum())\n",
    "\n",
    "print('=========================')      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Modeling:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BASIC MODELS: Fitting and Returning Scores for Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "* **Random Forest - Basic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiating a random forest model\n",
    "randf1_model1 = RandomForestClassifier(random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_train_scorer(randf1_model1, X_train_res, y_train_res, X_val_res, y_val_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Logistic Regression - Basic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiating a logistic regression model\n",
    "logreg1_model2 = LogisticRegression(random_state=3, max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_train_scorer(logreg1_model2, X_train_res, y_train_res, X_val_res, y_val_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Desision Search Tree - Basic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dectree1_model3 = DecisionTreeClassifier(random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_train_scorer(dectree1_model3, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Naive Bayes - Basic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf1_model4 = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_train_scorer(clf1_model4, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NON-BASIC MODELS: Fitting and Returning Scores for Models With Gridsearch CV and Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Logistic Regression - Pipeline & Gridsearch CV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(SMOTE(random_state=42), LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(pipe.get_params().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "                'logisticregression__C':[.5,.7,.8,.9,1.0],\n",
    "                'logisticregression__class_weight':[ { 0:.77, 1:.23}, None, \"balanced\" ],\n",
    "                'logisticregression__penalty': ['l1', 'l2'], \n",
    "                'logisticregression__solver': [ 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "                'logisticregression__max_iter':[100, 1000, 10000]\n",
    "            }\n",
    "\n",
    "            \n",
    "gs_logreg2_model5 = GridSearchCV(estimator=pipe, param_grid=param_grid, scoring = 'f1', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fitting to GS 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_train_scorer(gs_logreg2_model5, X_train_res, y_train_res, X_val_res, y_val_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
