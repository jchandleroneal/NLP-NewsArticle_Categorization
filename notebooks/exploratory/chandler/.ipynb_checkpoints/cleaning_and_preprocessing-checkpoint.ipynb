{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\">\n",
    "    <ul class=\"toc-item\">\n",
    "    <li><span><a href=\"#Cleaning the Data\" data-toc-modified-id=\"Cleaning the Data\">\n",
    "        <span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Cleaning the Data</a></span>    \n",
    "    <ul class=\"toc-item\">\n",
    "        <li><span><a href=\"#Importing the Data\" data-toc-modified-id=\"Importing the Data-1.1\">\n",
    "            <span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Importing the Data</a></span></li></ul></li>\n",
    "        <li><span><a href=\"#Inspecting the data\" data-toc-modified-id=\"Inspecting the data\">\n",
    "            <span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Inspecting the data</a></span>        \n",
    "    <ul class=\"toc-item\">\n",
    "        <li><span><a href=\"#Activation-functions-(from-before)\" data-toc-modified-id=\"Activation-functions-(from-before)-2.1\">\n",
    "              <span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Activation functions (from before)</a></span>  \n",
    "    <ul class=\"toc-item\">\n",
    "        <li><span><a href=\"#Recall-what-an-activation-function-does\" data-toc-modified-id=\"Recall-what-an-activation-function-does-2.1.1\">\n",
    "              <span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Recall what an activation function does</a></span></li>\n",
    "        <li><span><a href=\"#Using-the-proper-activation\" data-toc-modified-id=\"Using-the-proper-activation-2.1.2\">\n",
    "              <span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Using the proper activation</a></span></li></ul></li>\n",
    "        <li><span><a href=\"#Random-Starts\" data-toc-modified-id=\"Random-Starts-2.2\">\n",
    "              <span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Random Starts</a></span></li>\n",
    "        <li><span><a href=\"#Momentum\" data-toc-modified-id=\"Momentum-2.3\">\n",
    "              <span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Momentum</a></span>           \n",
    "    <ul class=\"toc-item\">\n",
    "        <li><span><a href=\"#Solution?--Give-it-some-speed-ðŸ˜Ž\" data-toc-modified-id=\"Solution?--Give-it-some-speed-ðŸ˜Ž-2.3.1\">\n",
    "              <span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>Solution?  Give it some speed ðŸ˜Ž</a></span><ul class=\"toc-item\">\n",
    "        <li><span><a href=\"#Code-Implementation\" data-toc-modified-id=\"Code-Implementation-2.3.1.1\">\n",
    "              <span class=\"toc-item-num\">2.3.1.1&nbsp;&nbsp;</span>Code Implementation</a></span></li></ul></li></ul></li></ul></li>\n",
    "        <li><span><a href=\"#Make-it-Go-Faster-with-Optimizers!\" data-toc-modified-id=\"Make-it-Go-Faster-with-Optimizers!-3\">\n",
    "              <span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Make it Go Faster with Optimizers!</a></span>            \n",
    "    <ul class=\"toc-item\">\n",
    "        <li><span><a href=\"#Normalization-(Batch-Normalization)\" data-toc-modified-id=\"Normalization-(Batch-Normalization)-3.1\">\n",
    "            <span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Normalization (Batch Normalization)</a></span></li>\n",
    "        <li><span><a href=\"#Stochastic-Gradient-Descent\" data-toc-modified-id=\"Stochastic-Gradient-Descent-3.2\">\n",
    "            <span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Stochastic Gradient Descent</a></span><ul class=\"toc-item\">\n",
    "        <li><span><a href=\"#Steps\" data-toc-modified-id=\"Steps-3.2.1\">\n",
    "            <span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Steps</a></span></li></ul></li>\n",
    "        <li><span><a href=\"#AdaGrad-&amp;-RMSProp\" data-toc-modified-id=\"AdaGrad-&amp;-RMSProp-3.3\">\n",
    "            <span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>AdaGrad &amp; RMSProp</a></span>                        \n",
    "    <ul class=\"toc-item\">\n",
    "        <li><span><a href=\"#Code-Example\" data-toc-modified-id=\"Code-Example-3.3.1\">\n",
    "            <span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>Code Example</a></span></li></ul></li>\n",
    "        <li><span><a href=\"#Adam-(Adaptivr-Moment-Estimation)-Optimization-and-Other-Variants\" data-toc-modified-id=\"Adam-(Adaptivr-Moment-\n",
    "                          Estimation)-Optimization-and-Other-Variants-3.4\">\n",
    "            <span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Adam (Adaptivr Moment Estimation) Optimization and Other Variants</a></span></li>          <li><span><a href=\"#Learning-Rate-Decay\" data-toc-modified-id=\"Learning-Rate-Decay-3.5\">\n",
    "            <span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Learning Rate Decay</a></span><ul class=\"toc-item\">\n",
    "        <li><span><a href=\"#Solution?\" data-toc-modified-id=\"Solution?-3.5.1\">\n",
    "            <span class=\"toc-item-num\">3.5.1&nbsp;&nbsp;</span>Solution?</a></span></li>\n",
    "        <li><span><a href=\"#Code-Example\" data-toc-modified-id=\"Code-Example-3.5.2\">\n",
    "            <span class=\"toc-item-num\">3.5.2&nbsp;&nbsp;</span>Code Example</a></span></li>\n",
    "</ul>\n",
    "</li>\n",
    "        \n",
    "</ul>\n",
    "</li>\n",
    "        \n",
    "</ul>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Cleaning the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>headline</th>\n",
       "      <th>authors</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CRIME</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>Melissa Jeltsen</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
       "      <td>She left her husband. He killed their children...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>Andy McDonald</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/will-smit...</td>\n",
       "      <td>Of course it has a song.</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 57</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/hugh-gran...</td>\n",
       "      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/jim-carre...</td>\n",
       "      <td>The actor gives Dems an ass-kicking for not fi...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/julianna-...</td>\n",
       "      <td>The \"Dietland\" actress said using the bags is ...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                           headline  \\\n",
       "0          CRIME  There Were 2 Mass Shootings In Texas Last Week...   \n",
       "1  ENTERTAINMENT  Will Smith Joins Diplo And Nicky Jam For The 2...   \n",
       "2  ENTERTAINMENT    Hugh Grant Marries For The First Time At Age 57   \n",
       "3  ENTERTAINMENT  Jim Carrey Blasts 'Castrato' Adam Schiff And D...   \n",
       "4  ENTERTAINMENT  Julianna Margulies Uses Donald Trump Poop Bags...   \n",
       "\n",
       "           authors                                               link  \\\n",
       "0  Melissa Jeltsen  https://www.huffingtonpost.com/entry/texas-ama...   \n",
       "1    Andy McDonald  https://www.huffingtonpost.com/entry/will-smit...   \n",
       "2       Ron Dicker  https://www.huffingtonpost.com/entry/hugh-gran...   \n",
       "3       Ron Dicker  https://www.huffingtonpost.com/entry/jim-carre...   \n",
       "4       Ron Dicker  https://www.huffingtonpost.com/entry/julianna-...   \n",
       "\n",
       "                                   short_description       date  \n",
       "0  She left her husband. He killed their children... 2018-05-26  \n",
       "1                           Of course it has a song. 2018-05-26  \n",
       "2  The actor and his longtime girlfriend Anna Ebe... 2018-05-26  \n",
       "3  The actor gives Dems an ass-kicking for not fi... 2018-05-26  \n",
       "4  The \"Dietland\" actress said using the bags is ... 2018-05-26  "
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd \n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline \n",
    "plt.style.use('fivethirtyeight')\n",
    "import nltk # importing Natural Landuage Processing Toolit\n",
    "\n",
    "from nltk.corpus import stopwords #importing stop words to remove non-necissary words\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import f1_score, confusion_matrix, get_scorer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "import re \n",
    "import nltk\n",
    "import string\n",
    "\n",
    "\n",
    "raw_data = pd.read_json('../../../data/data.json', lines=True)\n",
    "\n",
    "raw_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Inspecting the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200853 entries, 0 to 200852\n",
      "Data columns (total 6 columns):\n",
      " #   Column             Non-Null Count   Dtype         \n",
      "---  ------             --------------   -----         \n",
      " 0   category           200853 non-null  object        \n",
      " 1   headline           200853 non-null  object        \n",
      " 2   authors            200853 non-null  object        \n",
      " 3   link               200853 non-null  object        \n",
      " 4   short_description  200853 non-null  object        \n",
      " 5   date               200853 non-null  datetime64[ns]\n",
      "dtypes: datetime64[ns](1), object(5)\n",
      "memory usage: 9.2+ MB\n"
     ]
    }
   ],
   "source": [
    "#inspecting the data types - Noticed that they're all objects \n",
    "raw_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category             0\n",
       "headline             0\n",
       "authors              0\n",
       "link                 0\n",
       "short_description    0\n",
       "date                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking For Null Values\n",
    "raw_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>headline</th>\n",
       "      <th>authors</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CRIME</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>Melissa Jeltsen</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
       "      <td>She left her husband. He killed their children...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>Andy McDonald</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/will-smit...</td>\n",
       "      <td>Of course it has a song.</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 57</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/hugh-gran...</td>\n",
       "      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/jim-carre...</td>\n",
       "      <td>The actor gives Dems an ass-kicking for not fi...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/julianna-...</td>\n",
       "      <td>The \"Dietland\" actress said using the bags is ...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                           headline  \\\n",
       "0          CRIME  There Were 2 Mass Shootings In Texas Last Week...   \n",
       "1  ENTERTAINMENT  Will Smith Joins Diplo And Nicky Jam For The 2...   \n",
       "2  ENTERTAINMENT    Hugh Grant Marries For The First Time At Age 57   \n",
       "3  ENTERTAINMENT  Jim Carrey Blasts 'Castrato' Adam Schiff And D...   \n",
       "4  ENTERTAINMENT  Julianna Margulies Uses Donald Trump Poop Bags...   \n",
       "\n",
       "           authors                                               link  \\\n",
       "0  Melissa Jeltsen  https://www.huffingtonpost.com/entry/texas-ama...   \n",
       "1    Andy McDonald  https://www.huffingtonpost.com/entry/will-smit...   \n",
       "2       Ron Dicker  https://www.huffingtonpost.com/entry/hugh-gran...   \n",
       "3       Ron Dicker  https://www.huffingtonpost.com/entry/jim-carre...   \n",
       "4       Ron Dicker  https://www.huffingtonpost.com/entry/julianna-...   \n",
       "\n",
       "                                   short_description       date  \n",
       "0  She left her husband. He killed their children... 2018-05-26  \n",
       "1                           Of course it has a song. 2018-05-26  \n",
       "2  The actor and his longtime girlfriend Anna Ebe... 2018-05-26  \n",
       "3  The actor gives Dems an ass-kicking for not fi... 2018-05-26  \n",
       "4  The \"Dietland\" actress said using the bags is ... 2018-05-26  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reassigning dataframe to df to uphold the original structure of the data - used for later references.\n",
    "df = raw_data.copy()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the Data for Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Punctuation and cleaning rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Short Description Feature**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         She left her husband. He killed their children...\n",
       "1                                  Of course it has a song.\n",
       "2         The actor and his longtime girlfriend Anna Ebe...\n",
       "3         The actor gives Dems an ass-kicking for not fi...\n",
       "4         The \"Dietland\" actress said using the bags is ...\n",
       "                                ...                        \n",
       "200848    Verizon Wireless and AT&T are already promotin...\n",
       "200849    Afterward, Azarenka, more effusive with the pr...\n",
       "200850    Leading up to Super Bowl XLVI, the most talked...\n",
       "200851    CORRECTION: An earlier version of this story i...\n",
       "200852    The five-time all-star center tore into his te...\n",
       "Name: short_description, Length: 200853, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.short_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         There Were 2 Mass Shootings In Texas Last Week...\n",
       "1         Will Smith Joins Diplo And Nicky Jam For The 2...\n",
       "2           Hugh Grant Marries For The First Time At Age 57\n",
       "3         Jim Carrey Blasts 'Castrato' Adam Schiff And D...\n",
       "4         Julianna Margulies Uses Donald Trump Poop Bags...\n",
       "                                ...                        \n",
       "200848    RIM CEO Thorsten Heins' 'Significant' Plans Fo...\n",
       "200849    Maria Sharapova Stunned By Victoria Azarenka I...\n",
       "200850    Giants Over Patriots, Jets Over Colts Among  M...\n",
       "200851    Aldon Smith Arrested: 49ers Linebacker Busted ...\n",
       "200852    Dwight Howard Rips Teammates After Magic Loss ...\n",
       "Name: headline, Length: 200853, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.headline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing Casing and punctuaton**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    she left her husband he killed their children ...\n",
      "1                              of course it has a song\n",
      "2    the actor and his longtime girlfriend anna ebe...\n",
      "3    the actor gives dems an asskicking for not fig...\n",
      "4    the dietland actress said using the bags is a ...\n",
      "Name: short_description, dtype: object\n",
      "\n",
      "============================================================\n",
      "\n",
      " 0    there were 2 mass shootings in texas last week...\n",
      "1    will smith joins diplo and nicky jam for the 2...\n",
      "2      hugh grant marries for the first time at age 57\n",
      "3    jim carrey blasts castrato adam schiff and dem...\n",
      "4    julianna margulies uses donald trump poop bags...\n",
      "Name: headline, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Removing punctuation. It helps us reduce the size of the data \n",
    "df['short_description'] = df['short_description'].str.lower().str.replace('[^\\w\\s]','')\n",
    "print(df['short_description'].head(5))\n",
    "\n",
    "print('\\n============================================================')\n",
    "\n",
    "df['headline'] = df['headline'].str.lower().str.replace('[^\\w\\s]','')\n",
    "print('\\n', df['headline'].head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merging Description and Headline together**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* checking lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.short_description[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.headline[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#merging\n",
    "df['head_description'] = df.headline + ' ' + df.short_description\n",
    "len(df.head_description[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Removing Non-Contributional Words from the Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/chandler_oneal/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#downloading stopwords to use\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#inspecting stop words \n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>head_description</th>\n",
       "      <th>stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>there were 2 mass shootings in texas last week...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>will smith joins diplo and nicky jam for the 2...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hugh grant marries for the first time at age 5...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jim carrey blasts castrato adam schiff and dem...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>julianna margulies uses donald trump poop bags...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    head_description  stopwords\n",
       "0  there were 2 mass shootings in texas last week...         12\n",
       "1  will smith joins diplo and nicky jam for the 2...          8\n",
       "2  hugh grant marries for the first time at age 5...          9\n",
       "3  jim carrey blasts castrato adam schiff and dem...          7\n",
       "4  julianna margulies uses donald trump poop bags...          8"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how many stop words do we have? \n",
    "df['stopwords'] = df['head_description'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "df[['head_description','stopwords']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>headline</th>\n",
       "      <th>authors</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "      <th>date</th>\n",
       "      <th>head_description</th>\n",
       "      <th>stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CRIME</td>\n",
       "      <td>there were 2 mass shootings in texas last week...</td>\n",
       "      <td>Melissa Jeltsen</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
       "      <td>she left her husband he killed their children ...</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>there were 2 mass shootings in texas last week...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>will smith joins diplo and nicky jam for the 2...</td>\n",
       "      <td>Andy McDonald</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/will-smit...</td>\n",
       "      <td>of course it has a song</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>will smith joins diplo and nicky jam for the 2...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                           headline  \\\n",
       "0          CRIME  there were 2 mass shootings in texas last week...   \n",
       "1  ENTERTAINMENT  will smith joins diplo and nicky jam for the 2...   \n",
       "\n",
       "           authors                                               link  \\\n",
       "0  Melissa Jeltsen  https://www.huffingtonpost.com/entry/texas-ama...   \n",
       "1    Andy McDonald  https://www.huffingtonpost.com/entry/will-smit...   \n",
       "\n",
       "                                   short_description       date  \\\n",
       "0  she left her husband he killed their children ... 2018-05-26   \n",
       "1                            of course it has a song 2018-05-26   \n",
       "\n",
       "                                    head_description  stopwords  \n",
       "0  there were 2 mass shootings in texas last week...         12  \n",
       "1  will smith joins diplo and nicky jam for the 2...          8  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspecting stop words column to identify the number of unnecissary words within each column\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing Stop Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing stopwords \n",
    "df['clean_head_description'] = df['head_description'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2 mass shootings texas last week 1 tv left hus...\n",
       "1    smith joins diplo nicky jam 2018 world cups of...\n",
       "2    hugh grant marries first time age 57 actor lon...\n",
       "3    jim carrey blasts castrato adam schiff democra...\n",
       "4    julianna margulies uses donald trump poop bags...\n",
       "Name: clean_head_description, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_head_description'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing the Text to Up Sentences Into a List of Words:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **splitting a phrase, sentence, paragraph, or an entire text document into smaller units, such as individual words or terms. Each of these smaller units are called tokens.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmitizing the data to reduce repitition by removing the affixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiating a lemmatizer\n",
    "lemma = WordNetLemmatizer() #instantiate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing affixes from all words\n",
    "def lemm_words(data):\n",
    "    tokens = word_tokenize(data)   #Tokenizing data \n",
    "    lemmed_tokens = [lemma.lemmatize(word) for word in tokens]   #lemmatizing data\n",
    "    return ' '.join(lemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmed = df.head_description.apply(lemm_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = lemmed                              #there were 2 mass shooting in texas last week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing series into a list of lists \n",
    "df['unique_word'] = l.str.split(' ')   # [there, were, 2, mass, shooting, in, texas, la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[there, were, 2, mass, shooting, in, texas, la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[will, smith, join, diplo, and, nicky, jam, fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[hugh, grant, marries, for, the, first, time, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[jim, carrey, blast, castrato, adam, schiff, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[julianna, margulies, us, donald, trump, poop,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200848</th>\n",
       "      <td>[rim, ceo, thorsten, heins, significant, plan,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200849</th>\n",
       "      <td>[maria, sharapova, stunned, by, victoria, azar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200850</th>\n",
       "      <td>[giant, over, patriot, jet, over, colt, among,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200851</th>\n",
       "      <td>[aldon, smith, arrested, 49ers, linebacker, bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200852</th>\n",
       "      <td>[dwight, howard, rip, teammate, after, magic, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200853 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              unique_word\n",
       "0       [there, were, 2, mass, shooting, in, texas, la...\n",
       "1       [will, smith, join, diplo, and, nicky, jam, fo...\n",
       "2       [hugh, grant, marries, for, the, first, time, ...\n",
       "3       [jim, carrey, blast, castrato, adam, schiff, a...\n",
       "4       [julianna, margulies, us, donald, trump, poop,...\n",
       "...                                                   ...\n",
       "200848  [rim, ceo, thorsten, heins, significant, plan,...\n",
       "200849  [maria, sharapova, stunned, by, victoria, azar...\n",
       "200850  [giant, over, patriot, jet, over, colt, among,...\n",
       "200851  [aldon, smith, arrested, 49ers, linebacker, bu...\n",
       "200852  [dwight, howard, rip, teammate, after, magic, ...\n",
       "\n",
       "[200853 rows x 1 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['unique_word']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Creating a Bag of Words for Vectorization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list containing all words \n",
    "\n",
    "unique_lemmed_list = []                #the  (appears ->)  261830\n",
    "for x in df['unique_word']:  \n",
    "    unique_lemmed_list += x  # append elements of lists to full list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Removing Single Characters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5855301"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#inspecting the original number of words\n",
    "len(unique_lemmed_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looping through and adding words > len(1) to a new list \n",
    "full_words = [] # creating quotes \n",
    "index = 0\n",
    "for words in unique_lemmed_list: \n",
    "    if len(words) > 1:\n",
    "        full_words.append(words)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5621546"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking the NEW number of words after removal\n",
    "len(full_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Changing New Bag of Words into a Series for TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "the          261830\n",
       "to           163957\n",
       "of           128119\n",
       "and          119623\n",
       "in            95541\n",
       "              ...  \n",
       "samabaj           1\n",
       "beeeswax          1\n",
       "aeropress         1\n",
       "neutra            1\n",
       "gellman           1\n",
       "Length: 102078, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_value_counts = pd.Series(full_words).value_counts()\n",
    "word_value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>261830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>to</td>\n",
       "      <td>163957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>of</td>\n",
       "      <td>128119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>and</td>\n",
       "      <td>119623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>in</td>\n",
       "      <td>95541</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  word   count\n",
       "0  the  261830\n",
       "1   to  163957\n",
       "2   of  128119\n",
       "3  and  119623\n",
       "4   in   95541"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot = pd.DataFrame(data = word_value_counts).reset_index()\n",
    "plot.columns = ['word','count']\n",
    "plot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to do --- calculating sentament of text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merging Similar Columns:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>headline</th>\n",
       "      <th>authors</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "      <th>date</th>\n",
       "      <th>head_description</th>\n",
       "      <th>stopwords</th>\n",
       "      <th>clean_head_description</th>\n",
       "      <th>unique_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GENERAL</td>\n",
       "      <td>there were 2 mass shootings in texas last week...</td>\n",
       "      <td>Melissa Jeltsen</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
       "      <td>she left her husband he killed their children ...</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>there were 2 mass shootings in texas last week...</td>\n",
       "      <td>12</td>\n",
       "      <td>2 mass shootings texas last week 1 tv left hus...</td>\n",
       "      <td>[there, were, 2, mass, shooting, in, texas, la...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  category                                           headline  \\\n",
       "0  GENERAL  there were 2 mass shootings in texas last week...   \n",
       "\n",
       "           authors                                               link  \\\n",
       "0  Melissa Jeltsen  https://www.huffingtonpost.com/entry/texas-ama...   \n",
       "\n",
       "                                   short_description       date  \\\n",
       "0  she left her husband he killed their children ... 2018-05-26   \n",
       "\n",
       "                                    head_description  stopwords  \\\n",
       "0  there were 2 mass shootings in texas last week...         12   \n",
       "\n",
       "                              clean_head_description  \\\n",
       "0  2 mass shootings texas last week 1 tv left hus...   \n",
       "\n",
       "                                         unique_word  \n",
       "0  [there, were, 2, mass, shooting, in, texas, la...  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Visualizing current columns**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of categories: 12\n",
      "\n",
      "\n",
      "Listing Categories: LIFESTYLE AND WELLNESS          40619\n",
      "POLITICS                        32739\n",
      "SPORTS AND ENTERTAINMENT        30296\n",
      "TRAVEL-TOURISM & ART-CULTURE    20578\n",
      "EMPOWERED VOICES                15461\n",
      "PARENTING AND EDUCATION         14780\n",
      "MISC                            12448\n",
      "GENERAL                          9663\n",
      "WORLDNEWS                        8420\n",
      "BUSINESS-MONEY                   7644\n",
      "SCIENCE AND TECH                 4260\n",
      "ENVIRONMENT                      3945\n",
      "Name: category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#total number of categories\n",
    "print('# of categories:', df.category.value_counts().nunique())\n",
    "\n",
    "print('\\n\\nListing Categories:', df.category.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Combining columns**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['LIFESTYLE AND WELLNESS', 'POLITICS', 'SPORTS AND ENTERTAINMENT',\n",
       "       'TRAVEL-TOURISM & ART-CULTURE', 'EMPOWERED VOICES',\n",
       "       'PARENTING AND EDUCATION', 'MISC', 'GENERAL', 'WORLDNEWS',\n",
       "       'BUSINESS-MONEY', 'SCIENCE AND TECH', 'ENVIRONMENT'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = df['category'].value_counts().index\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupper(grouplist,name):\n",
    "    for idx in categories: #for each category name \n",
    "        if idx in grouplist: #if the category name appears in the grouplist below \n",
    "            df.loc[df['category'] == idx, 'category'] = name  #assign it to name = \" \"\n",
    "\n",
    "\n",
    "\n",
    "groupper( grouplist= ['WELLNESS', 'HEALTHY LIVING','HOME & LIVING','STYLE & BEAUTY' ,'STYLE'] , name =  'LIFESTYLE AND WELLNESS')\n",
    "groupper( grouplist= [ 'PARENTING', 'PARENTS' ,'EDUCATION' ,'COLLEGE'] , name =  'PARENTING AND EDUCATION')\n",
    "groupper( grouplist= ['SPORTS','ENTERTAINMENT' , 'COMEDY','WEIRD NEWS','ARTS'] , name =  'SPORTS AND ENTERTAINMENT')\n",
    "groupper( grouplist= ['TRAVEL', 'ARTS & CULTURE','CULTURE & ARTS','FOOD & DRINK', 'TASTE'] , name =  'TRAVEL-TOURISM & ART-CULTURE')\n",
    "groupper( grouplist= ['WOMEN','QUEER VOICES', 'LATINO VOICES', 'BLACK VOICES'] , name =  'EMPOWERED VOICES')\n",
    "groupper( grouplist= ['BUSINESS' ,  'MONEY'] , name =  'BUSINESS-MONEY')\n",
    "groupper( grouplist= ['THE WORLDPOST' , 'WORLDPOST' , 'WORLD NEWS'] , name =  'WORLDNEWS')\n",
    "groupper( grouplist= ['ENVIRONMENT' ,'GREEN'] , name =  'ENVIRONMENT')\n",
    "groupper( grouplist= ['TECH', 'SCIENCE'] , name =  'SCIENCE AND TECH')\n",
    "groupper( grouplist= ['FIFTY' , 'IMPACT' ,'GOOD NEWS','CRIME'] , name =  'GENERAL')\n",
    "groupper( grouplist= ['WEDDINGS', 'DIVORCE',  'RELIGION','MEDIA'] , name =  'MISC')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Inspecting new columns**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of categories: 12\n",
      "\n",
      "\n",
      "Listing Categories: LIFESTYLE AND WELLNESS          40619\n",
      "POLITICS                        32739\n",
      "SPORTS AND ENTERTAINMENT        30296\n",
      "TRAVEL-TOURISM & ART-CULTURE    20578\n",
      "EMPOWERED VOICES                15461\n",
      "PARENTING AND EDUCATION         14780\n",
      "MISC                            12448\n",
      "GENERAL                          9663\n",
      "WORLDNEWS                        8420\n",
      "BUSINESS-MONEY                   7644\n",
      "SCIENCE AND TECH                 4260\n",
      "ENVIRONMENT                      3945\n",
      "Name: category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#inspecting newly combined categories\n",
    "print('# of categories:', df.category.value_counts().nunique())\n",
    "print('\\n\\nListing Categories:', df.category.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **TF - IDF**\n",
    "\n",
    "**Term Frequency â€” Inverse Document Frequency** - Derermining word importance to documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Vectorizing the (title description + full description) column (converting it into numeric data).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a function to pass into the analyzer to clean the raw data \n",
    "    \n",
    "def clean_history(history):\n",
    "    history = \"\".join([word for word in history if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', history)\n",
    "    history = [wn.lemmatize(word) for word in tokens if word not in stopwords]\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returning Trigrams - Groups of Three Words\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=150, ngram_range =(3, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf = tfidf_vectorizer.fit_transform(df['head_description'])\n",
    "features = (tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['according to the',\n",
       " 'across the country',\n",
       " 'all of the',\n",
       " 'all of us',\n",
       " 'and how to',\n",
       " 'and instagram at',\n",
       " 'around the world',\n",
       " 'as much as',\n",
       " 'as well as',\n",
       " 'at the end',\n",
       " 'be able to',\n",
       " 'be sure to',\n",
       " 'check out huffpost',\n",
       " 'check out the',\n",
       " 'do you have',\n",
       " 'donald trump is',\n",
       " 'dont have to',\n",
       " 'dont want to',\n",
       " 'end of the',\n",
       " 'facebook tumblr pinterest',\n",
       " 'for the first',\n",
       " 'game of thrones',\n",
       " 'going to be',\n",
       " 'have to be',\n",
       " 'here are some',\n",
       " 'here are the',\n",
       " 'how do you',\n",
       " 'how to get',\n",
       " 'how to make',\n",
       " 'huffpost style on',\n",
       " 'if you are',\n",
       " 'if you have',\n",
       " 'if you want',\n",
       " 'in front of',\n",
       " 'in honor of',\n",
       " 'in new york',\n",
       " 'in order to',\n",
       " 'in the face',\n",
       " 'in the middle',\n",
       " 'in the new',\n",
       " 'in the past',\n",
       " 'in the united',\n",
       " 'in the us',\n",
       " 'in the world',\n",
       " 'in touch check',\n",
       " 'in your life',\n",
       " 'instagram at huffpoststyle',\n",
       " 'is going to',\n",
       " 'is not the',\n",
       " 'is one of',\n",
       " 'is the best',\n",
       " 'is the most',\n",
       " 'it can be',\n",
       " 'it comes to',\n",
       " 'it is not',\n",
       " 'it was the',\n",
       " 'it would be',\n",
       " 'its time to',\n",
       " 'keep in touch',\n",
       " 'look at the',\n",
       " 'many of us',\n",
       " 'may not be',\n",
       " 'more be sure',\n",
       " 'more likely to',\n",
       " 'most of the',\n",
       " 'most of us',\n",
       " 'need to be',\n",
       " 'need to know',\n",
       " 'new york city',\n",
       " 'new york fashion',\n",
       " 'new york times',\n",
       " 'no matter how',\n",
       " 'of the best',\n",
       " 'of the day',\n",
       " 'of the most',\n",
       " 'of the week',\n",
       " 'of the world',\n",
       " 'of the worlds',\n",
       " 'of the year',\n",
       " 'on twitter facebook',\n",
       " 'one of the',\n",
       " 'open letter to',\n",
       " 'out huffpost style',\n",
       " 'out of the',\n",
       " 'over the past',\n",
       " 'part of the',\n",
       " 'photos want more',\n",
       " 'pinterest and instagram',\n",
       " 'read more on',\n",
       " 'rest of the',\n",
       " 'some of the',\n",
       " 'style on twitter',\n",
       " 'sure to check',\n",
       " 'the age of',\n",
       " 'the best way',\n",
       " 'the end of',\n",
       " 'the face of',\n",
       " 'the fact that',\n",
       " 'the first time',\n",
       " 'the future of',\n",
       " 'the importance of',\n",
       " 'the most important',\n",
       " 'the new year',\n",
       " 'the new york',\n",
       " 'the opportunity to',\n",
       " 'the power of',\n",
       " 'the rest of',\n",
       " 'the slideshow below',\n",
       " 'the supreme court',\n",
       " 'the united states',\n",
       " 'the white house',\n",
       " 'there is no',\n",
       " 'these are the',\n",
       " 'this is not',\n",
       " 'this is the',\n",
       " 'this is what',\n",
       " 'time of year',\n",
       " 'to be the',\n",
       " 'to check out',\n",
       " 'to deal with',\n",
       " 'to do with',\n",
       " 'to help you',\n",
       " 'to know about',\n",
       " 'to make it',\n",
       " 'to make the',\n",
       " 'to see the',\n",
       " 'to talk about',\n",
       " 'touch check out',\n",
       " 'tumblr pinterest and',\n",
       " 'twitter facebook tumblr',\n",
       " 'want more be',\n",
       " 'want to be',\n",
       " 'we have to',\n",
       " 'we need to',\n",
       " 'we want to',\n",
       " 'welcome to the',\n",
       " 'what do you',\n",
       " 'what happens when',\n",
       " 'what to do',\n",
       " 'what you need',\n",
       " 'when it comes',\n",
       " 'why you should',\n",
       " 'will make you',\n",
       " 'york fashion week',\n",
       " 'you can do',\n",
       " 'you dont have',\n",
       " 'you have to',\n",
       " 'you need to',\n",
       " 'you want to',\n",
       " 'your life and']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Shape of tfidf : \n",
      " (200853, 150)\n",
      "\n",
      "\n",
      "Features : \n",
      " ['according to the', 'across the country', 'all of the', 'all of us', 'and how to', 'and instagram at', 'around the world', 'as much as', 'as well as', 'at the end', 'be able to', 'be sure to', 'check out huffpost', 'check out the', 'do you have', 'donald trump is', 'dont have to', 'dont want to', 'end of the', 'facebook tumblr pinterest', 'for the first', 'game of thrones', 'going to be', 'have to be', 'here are some', 'here are the', 'how do you', 'how to get', 'how to make', 'huffpost style on', 'if you are', 'if you have', 'if you want', 'in front of', 'in honor of', 'in new york', 'in order to', 'in the face', 'in the middle', 'in the new', 'in the past', 'in the united', 'in the us', 'in the world', 'in touch check', 'in your life', 'instagram at huffpoststyle', 'is going to', 'is not the', 'is one of', 'is the best', 'is the most', 'it can be', 'it comes to', 'it is not', 'it was the', 'it would be', 'its time to', 'keep in touch', 'look at the', 'many of us', 'may not be', 'more be sure', 'more likely to', 'most of the', 'most of us', 'need to be', 'need to know', 'new york city', 'new york fashion', 'new york times', 'no matter how', 'of the best', 'of the day', 'of the most', 'of the week', 'of the world', 'of the worlds', 'of the year', 'on twitter facebook', 'one of the', 'open letter to', 'out huffpost style', 'out of the', 'over the past', 'part of the', 'photos want more', 'pinterest and instagram', 'read more on', 'rest of the', 'some of the', 'style on twitter', 'sure to check', 'the age of', 'the best way', 'the end of', 'the face of', 'the fact that', 'the first time', 'the future of', 'the importance of', 'the most important', 'the new year', 'the new york', 'the opportunity to', 'the power of', 'the rest of', 'the slideshow below', 'the supreme court', 'the united states', 'the white house', 'there is no', 'these are the', 'this is not', 'this is the', 'this is what', 'time of year', 'to be the', 'to check out', 'to deal with', 'to do with', 'to help you', 'to know about', 'to make it', 'to make the', 'to see the', 'to talk about', 'touch check out', 'tumblr pinterest and', 'twitter facebook tumblr', 'want more be', 'want to be', 'we have to', 'we need to', 'we want to', 'welcome to the', 'what do you', 'what happens when', 'what to do', 'what you need', 'when it comes', 'why you should', 'will make you', 'york fashion week', 'you can do', 'you dont have', 'you have to', 'you need to', 'you want to', 'your life and']\n",
      "\n",
      "\n",
      "X1 : \n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\nShape of tfidf : \\n\", X_tfidf.shape)\n",
    "\n",
    "\n",
    "#printing out the features \n",
    "print(\"\\n\\nFeatures : \\n\", features)\n",
    "print(\"\\n\\nX1 : \\n\", X_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=features)\n",
    "# X_tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    150\n",
       "Name: 100, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#inspecting values for imbalances to determine whether or not to SMOTE the data in the upcoming pipeline\n",
    "X_tfidf_df.iloc[100].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Preparatition:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating a function to shorten the modeling process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a function to pass in our model that will then return an F-1 Score\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "\n",
    "def make_train_scorer(model, X_train, y_train):\n",
    "    model.fit(X_train, y_train)\n",
    "    scorer = make_scorer(f1_score, average='weighted')\n",
    "    \n",
    "    train_f1 = scorer(model, X_train, y_train)\n",
    "    val_f1 = scorer(model, X_val, y_val)\n",
    "    \n",
    "    print('Train Score:', train_f1)\n",
    "    print('Validation Score:', val_f1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_test_scorer(model, X_test y_test):    \n",
    "    model.fit(X_train, y_train)\n",
    "    scorer = make_scorer(f1_score, average='weighted')\n",
    "    \n",
    "    test_f1 = scorer(model, X_test, y_test)\n",
    "    print('Test Score:', test_f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Splitting the vecorized data into the variable X and the Categories into the variable y**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_tfidf_df\n",
    "y = df.category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_test, y_tr, y_test = train_test_split(X, y, random_state=3, test_size=.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_tr, y_tr, random_state=3, test_size=.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Modeling:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BASIC MODELS: Fitting and Returning Scores for Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Random Forest - Basic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiating a random forest model\n",
    "randf1_model1 = RandomForestClassifier(random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.13708800529317708\n",
      "Validation Score: 0.11207604641149277\n"
     ]
    }
   ],
   "source": [
    "make_train_scorer(randf1_model1, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Logistic Regression - Basic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiating a logistic regression model\n",
    "logreg1_model2 = LogisticRegression(random_state=3, max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.11068395097304644\n",
      "Validation Score: 0.11030909396069086\n"
     ]
    }
   ],
   "source": [
    "make_train_scorer(logreg1_model2, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Desision Search Tree - Basic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "dectree1_model3 = DecisionTreeClassifier(random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.13689551487268894\n",
      "Validation Score: 0.11151721390253287\n"
     ]
    }
   ],
   "source": [
    "make_train_scorer(dectree1_model3, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NON-BASIC MODELS: Fitting and Returning Scores for Models With Gridsearch CV and Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(SMOTE(random_state=42), RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['memory',\n",
       " 'steps',\n",
       " 'verbose',\n",
       " 'smote',\n",
       " 'randomforestclassifier',\n",
       " 'smote__k_neighbors',\n",
       " 'smote__n_jobs',\n",
       " 'smote__random_state',\n",
       " 'smote__sampling_strategy',\n",
       " 'randomforestclassifier__bootstrap',\n",
       " 'randomforestclassifier__ccp_alpha',\n",
       " 'randomforestclassifier__class_weight',\n",
       " 'randomforestclassifier__criterion',\n",
       " 'randomforestclassifier__max_depth',\n",
       " 'randomforestclassifier__max_features',\n",
       " 'randomforestclassifier__max_leaf_nodes',\n",
       " 'randomforestclassifier__max_samples',\n",
       " 'randomforestclassifier__min_impurity_decrease',\n",
       " 'randomforestclassifier__min_impurity_split',\n",
       " 'randomforestclassifier__min_samples_leaf',\n",
       " 'randomforestclassifier__min_samples_split',\n",
       " 'randomforestclassifier__min_weight_fraction_leaf',\n",
       " 'randomforestclassifier__n_estimators',\n",
       " 'randomforestclassifier__n_jobs',\n",
       " 'randomforestclassifier__oob_score',\n",
       " 'randomforestclassifier__random_state',\n",
       " 'randomforestclassifier__verbose',\n",
       " 'randomforestclassifier__warm_start']"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(pipe.get_params().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating GridSearch CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "                'randomforestclassifier__criterion':['gini','entropy'],\n",
    "                'randomforestclassifier__max_depth':[2,3,4,5,6,7,8,9,10],\n",
    "                'randomforestclassifier__max_leaf_nodes':[2,3,4,5,6,7,8,9,10,11,12],\n",
    "                'randomforestclassifier__n_estimators':[10,20,30,40,50,60,70,80,80,100]\n",
    "              }\n",
    "            \n",
    "gs_logreg2_model4 = GridSearchCV(estimator=pipe, param_grid=param_grid, scoring = 'f1', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fitting to GS 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_train_scorer(gs_logreg2_model4, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
