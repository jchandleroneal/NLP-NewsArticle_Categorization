{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.layers import Embedding\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>headline</th>\n",
       "      <th>short_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CRIME</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>She left her husband. He killed their children...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>Of course it has a song.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 57</td>\n",
       "      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>The actor gives Dems an ass-kicking for not fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>The \"Dietland\" actress said using the bags is ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                           headline  \\\n",
       "0          CRIME  There Were 2 Mass Shootings In Texas Last Week...   \n",
       "1  ENTERTAINMENT  Will Smith Joins Diplo And Nicky Jam For The 2...   \n",
       "2  ENTERTAINMENT    Hugh Grant Marries For The First Time At Age 57   \n",
       "3  ENTERTAINMENT  Jim Carrey Blasts 'Castrato' Adam Schiff And D...   \n",
       "4  ENTERTAINMENT  Julianna Margulies Uses Donald Trump Poop Bags...   \n",
       "\n",
       "                                   short_description  \n",
       "0  She left her husband. He killed their children...  \n",
       "1                           Of course it has a song.  \n",
       "2  The actor and his longtime girlfriend Anna Ebe...  \n",
       "3  The actor gives Dems an ass-kicking for not fi...  \n",
       "4  The \"Dietland\" actress said using the bags is ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json('../../../data/data.json', lines=True)\n",
    "df.drop(['authors', 'link', 'date'], axis = 1, inplace = True) \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = df['category'].value_counts().index\n",
    "\n",
    "def groupper(grouplist,name):\n",
    "    for ele in categories:\n",
    "        if ele in grouplist:\n",
    "            df.loc[df['category'] == ele, 'category'] = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupper( grouplist= ['WELLNESS', 'HEALTHY LIVING','HOME & LIVING','STYLE & BEAUTY' ,'STYLE'] , name =  'LIFESTYLE AND WELLNESS')\n",
    "\n",
    "groupper( grouplist= [ 'PARENTING', 'PARENTS' ,'EDUCATION' ,'COLLEGE'] , name =  'PARENTING AND EDUCATION')\n",
    "\n",
    "groupper( grouplist= ['SPORTS','ENTERTAINMENT' , 'COMEDY','WEIRD NEWS','ARTS'] , name =  'SPORTS AND ENTERTAINMENT')\n",
    "\n",
    "groupper( grouplist= ['TRAVEL', 'ARTS & CULTURE','CULTURE & ARTS','FOOD & DRINK', 'TASTE'] , name =  'TRAVEL-TOURISM & ART-CULTURE')\n",
    "\n",
    "groupper( grouplist= ['WOMEN','QUEER VOICES', 'LATINO VOICES', 'BLACK VOICES'] , name =  'EMPOWERED VOICES')\n",
    "\n",
    "groupper( grouplist= ['BUSINESS' ,  'MONEY'] , name =  'BUSINESS-MONEY')\n",
    "\n",
    "groupper( grouplist= ['THE WORLDPOST' , 'WORLDPOST' , 'WORLD NEWS'] , name =  'WORLDNEWS')\n",
    "\n",
    "groupper( grouplist= ['ENVIRONMENT' ,'GREEN'] , name =  'ENVIRONMENT')\n",
    "\n",
    "groupper( grouplist= ['TECH', 'SCIENCE'] , name =  'SCIENCE AND TECH')\n",
    "\n",
    "groupper( grouplist= ['FIFTY' , 'IMPACT' ,'GOOD NEWS','CRIME'] , name =  'GENERAL')\n",
    "\n",
    "groupper( grouplist= ['WEDDINGS', 'DIVORCE',  'RELIGION','MEDIA'] , name =  'MISC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a total of 12 categories now\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LIFESTYLE AND WELLNESS          40619\n",
       "POLITICS                        32739\n",
       "SPORTS AND ENTERTAINMENT        30296\n",
       "TRAVEL-TOURISM & ART-CULTURE    20578\n",
       "EMPOWERED VOICES                15461\n",
       "PARENTING AND EDUCATION         14780\n",
       "MISC                            12448\n",
       "GENERAL                          9663\n",
       "WORLDNEWS                        8420\n",
       "BUSINESS-MONEY                   7644\n",
       "SCIENCE AND TECH                 4260\n",
       "ENVIRONMENT                      3945\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"We have a total of {} categories now\".format(df['category'].nunique()))\n",
    "df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19712\n"
     ]
    }
   ],
   "source": [
    "print(len(df2[df2['short_description'] == \"\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GENERAL</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SPORTS AND ENTERTAINMENT</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SPORTS AND ENTERTAINMENT</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SPORTS AND ENTERTAINMENT</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SPORTS AND ENTERTAINMENT</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200848</th>\n",
       "      <td>SCIENCE AND TECH</td>\n",
       "      <td>RIM CEO Thorsten Heins' 'Significant' Plans Fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200849</th>\n",
       "      <td>SPORTS AND ENTERTAINMENT</td>\n",
       "      <td>Maria Sharapova Stunned By Victoria Azarenka I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200850</th>\n",
       "      <td>SPORTS AND ENTERTAINMENT</td>\n",
       "      <td>Giants Over Patriots, Jets Over Colts Among  M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200851</th>\n",
       "      <td>SPORTS AND ENTERTAINMENT</td>\n",
       "      <td>Aldon Smith Arrested: 49ers Linebacker Busted ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200852</th>\n",
       "      <td>SPORTS AND ENTERTAINMENT</td>\n",
       "      <td>Dwight Howard Rips Teammates After Magic Loss ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200853 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        category  \\\n",
       "0                        GENERAL   \n",
       "1       SPORTS AND ENTERTAINMENT   \n",
       "2       SPORTS AND ENTERTAINMENT   \n",
       "3       SPORTS AND ENTERTAINMENT   \n",
       "4       SPORTS AND ENTERTAINMENT   \n",
       "...                          ...   \n",
       "200848          SCIENCE AND TECH   \n",
       "200849  SPORTS AND ENTERTAINMENT   \n",
       "200850  SPORTS AND ENTERTAINMENT   \n",
       "200851  SPORTS AND ENTERTAINMENT   \n",
       "200852  SPORTS AND ENTERTAINMENT   \n",
       "\n",
       "                                                     text  \n",
       "0       There Were 2 Mass Shootings In Texas Last Week...  \n",
       "1       Will Smith Joins Diplo And Nicky Jam For The 2...  \n",
       "2       Hugh Grant Marries For The First Time At Age 5...  \n",
       "3       Jim Carrey Blasts 'Castrato' Adam Schiff And D...  \n",
       "4       Julianna Margulies Uses Donald Trump Poop Bags...  \n",
       "...                                                   ...  \n",
       "200848  RIM CEO Thorsten Heins' 'Significant' Plans Fo...  \n",
       "200849  Maria Sharapova Stunned By Victoria Azarenka I...  \n",
       "200850  Giants Over Patriots, Jets Over Colts Among  M...  \n",
       "200851  Aldon Smith Arrested: 49ers Linebacker Busted ...  \n",
       "200852  Dwight Howard Rips Teammates After Magic Loss ...  \n",
       "\n",
       "[200853 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['text'] = df2['headline'].astype(str)+\"-\"+df2['short_description']\n",
    "df2.drop(columns =['headline','short_description'],axis = 1, inplace=True)\n",
    "df2.astype(str)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "df2 = shuffle(df2)\n",
    "df2.reset_index(inplace=True, drop=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df2['text']\n",
    "Y= df2['category']\n",
    "#80% to train , 10% for validation , 10% for testing\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X,Y, test_size=0.2, random_state=42)\n",
    "X_val, X_test , y_val, y_test= train_test_split(X_val,y_val, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size =20000\n",
    "max_length = 150\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<OOV>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160682, 150)\n",
      "(160682, 12)\n",
      "(20085, 150)\n",
      "(20085, 12)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_train = pad_sequences(X_train,maxlen= max_length,padding=padding_type, truncating=trunc_type)\n",
    "y_train = np.asarray(y_train)\n",
    "y_train = pd.get_dummies(y_train)\n",
    "\n",
    "X_val = tokenizer.texts_to_sequences(X_val)\n",
    "X_val = pad_sequences(X_val,maxlen= max_length,padding=padding_type, truncating=trunc_type)\n",
    "y_val = np.asarray(y_val)\n",
    "y_val = pd.get_dummies(y_val)\n",
    "\n",
    "train_set = np.array(X_train)\n",
    "val_set = np.array(X_val)\n",
    "\n",
    "train_label = np.array(y_train)\n",
    "val_label = np.array(y_val)\n",
    "\n",
    "\n",
    "y_test = pd.get_dummies(y_test)\n",
    "y_test = np.asarray(y_test)\n",
    "y_test = np.argmax(y_test,axis=1)   #this would be our ground truth label while testing\n",
    "\n",
    "print(train_set.shape)\n",
    "print(train_label.shape)\n",
    "\n",
    "\n",
    "print(val_set.shape)\n",
    "print(val_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_glove_file =  '../../../../../glove.6B.100d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400001 word vectors.\n",
      "Converted 66004 words (38804 misses)\n"
     ]
    }
   ],
   "source": [
    "#Initialising the embedding matrix with glove vec embeddings\n",
    "\n",
    "num_tokens = len(tokenizer.word_index.items()) + 2\n",
    "embedding_dim = 100\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file, encoding=\"utf8\" ) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))\n",
    "\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 100)         10481000  \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 64)                34048     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 12)                780       \n",
      "=================================================================\n",
      "Total params: 10,515,828\n",
      "Trainable params: 34,828\n",
      "Non-trainable params: 10,481,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "early_stop=tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                            patience=3, min_delta=0.0001)\n",
    "\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "embed_size = 100\n",
    "model = keras.models.Sequential([\n",
    "                                 \n",
    "        Embedding(num_tokens,\n",
    "        embedding_dim,\n",
    "        embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "        mask_zero=True,input_shape=[None],trainable=False),\n",
    "        keras.layers.Bidirectional(keras.layers.LSTM(32, dropout = 0.4)),\n",
    "        keras.layers.Dense(12, activation=\"softmax\")\n",
    "            \n",
    "        ])\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5021/5021 [==============================] - 345s 69ms/step - loss: 1.2954 - accuracy: 0.5842 - val_loss: 1.0726 - val_accuracy: 0.6552\n",
      "Epoch 2/20\n",
      "5021/5021 [==============================] - 364s 73ms/step - loss: 1.1080 - accuracy: 0.6450 - val_loss: 1.0247 - val_accuracy: 0.6674\n",
      "Epoch 3/20\n",
      "5021/5021 [==============================] - 369s 73ms/step - loss: 1.0676 - accuracy: 0.6569 - val_loss: 0.9992 - val_accuracy: 0.6755\n",
      "Epoch 4/20\n",
      "5021/5021 [==============================] - 376s 75ms/step - loss: 1.0439 - accuracy: 0.6652 - val_loss: 0.9884 - val_accuracy: 0.6824\n",
      "Epoch 5/20\n",
      "5021/5021 [==============================] - 364s 72ms/step - loss: 1.0295 - accuracy: 0.6684 - val_loss: 0.9741 - val_accuracy: 0.6887\n",
      "Epoch 6/20\n",
      "5021/5021 [==============================] - 371s 74ms/step - loss: 1.0213 - accuracy: 0.6712 - val_loss: 0.9643 - val_accuracy: 0.6902\n",
      "Epoch 7/20\n",
      "5021/5021 [==============================] - 396s 79ms/step - loss: 1.0117 - accuracy: 0.6738 - val_loss: 0.9657 - val_accuracy: 0.6887\n",
      "Epoch 8/20\n",
      "5021/5021 [==============================] - 361s 72ms/step - loss: 1.0054 - accuracy: 0.6759 - val_loss: 0.9565 - val_accuracy: 0.6923\n",
      "Epoch 9/20\n",
      "5021/5021 [==============================] - 367s 73ms/step - loss: 1.0029 - accuracy: 0.6770 - val_loss: 0.9563 - val_accuracy: 0.6914\n",
      "Epoch 10/20\n",
      "5021/5021 [==============================] - 371s 74ms/step - loss: 0.9973 - accuracy: 0.6777 - val_loss: 0.9521 - val_accuracy: 0.6928\n",
      "Epoch 11/20\n",
      "5021/5021 [==============================] - 415s 83ms/step - loss: 0.9942 - accuracy: 0.6784 - val_loss: 0.9520 - val_accuracy: 0.6937\n",
      "Epoch 12/20\n",
      "5021/5021 [==============================] - 426s 85ms/step - loss: 0.9882 - accuracy: 0.6803 - val_loss: 0.9499 - val_accuracy: 0.6961\n",
      "Epoch 13/20\n",
      "5021/5021 [==============================] - 418s 83ms/step - loss: 0.9870 - accuracy: 0.6805 - val_loss: 0.9508 - val_accuracy: 0.6962\n",
      "Epoch 14/20\n",
      "5021/5021 [==============================] - 353s 70ms/step - loss: 0.9849 - accuracy: 0.6802 - val_loss: 0.9454 - val_accuracy: 0.6973\n",
      "Epoch 15/20\n",
      "5021/5021 [==============================] - 329s 65ms/step - loss: 0.9842 - accuracy: 0.6819 - val_loss: 0.9479 - val_accuracy: 0.6965\n",
      "Epoch 16/20\n",
      "5021/5021 [==============================] - 393s 78ms/step - loss: 0.9796 - accuracy: 0.6831 - val_loss: 0.9427 - val_accuracy: 0.6981\n",
      "Epoch 17/20\n",
      "5021/5021 [==============================] - 374s 74ms/step - loss: 0.9785 - accuracy: 0.6839 - val_loss: 0.9446 - val_accuracy: 0.6955\n",
      "Epoch 18/20\n",
      "5021/5021 [==============================] - 323s 64ms/step - loss: 0.9756 - accuracy: 0.6848 - val_loss: 0.9444 - val_accuracy: 0.6948\n",
      "Epoch 19/20\n",
      "5021/5021 [==============================] - 365s 73ms/step - loss: 0.9750 - accuracy: 0.6850 - val_loss: 0.9404 - val_accuracy: 0.6967\n",
      "Epoch 20/20\n",
      "5021/5021 [==============================] - 384s 77ms/step - loss: 0.9733 - accuracy: 0.6842 - val_loss: 0.9346 - val_accuracy: 0.6995\n"
     ]
    }
   ],
   "source": [
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "history = model.fit( train_set,train_label,\n",
    "                     batch_size = 32,\n",
    "                     steps_per_epoch=len(X_train) // 32, \n",
    "                     validation_data = (val_set , val_label),\n",
    "                     validation_steps = len(val_set)//32, epochs=20,\n",
    "                     callbacks=  early_stop )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./best_model2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = keras.models.load_model('./best_model2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 0s/step - loss: 0.0000e+00 - accuracy: 0.0000e+00\n",
      "accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "text_1 = 'Pelosi: Biden Can’t Cancel Student Debt '\n",
    "text_2 = \"Feds Take on Student Homelessness\"\n",
    "score = model3.evaluate([text_1, text_2])\n",
    "print(\"%s: %.2f%%\" % (model3.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 230,   34,  100, 4933,  716, 6732,   89,  588,   11, 2709,  522,\n",
       "       1961,   49,  150,   89,  147,   72,   42,   58,    1,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.4872001e-04, 7.5347912e-01, 3.0899842e-05, ..., 2.0094652e-02,\n",
       "        3.6069824e-04, 2.7039887e-03],\n",
       "       [7.4096411e-03, 4.3876776e-01, 3.2646044e-03, ..., 1.1931347e-01,\n",
       "        1.8968061e-02, 5.4024428e-02],\n",
       "       [1.4406631e-02, 1.2106130e-02, 2.3615919e-03, ..., 1.3777937e-01,\n",
       "        6.7087388e-01, 1.9020332e-02],\n",
       "       ...,\n",
       "       [5.2680145e-03, 4.7150757e-02, 2.5488217e-03, ..., 2.5438381e-02,\n",
       "        7.8933220e-03, 7.8599136e-03],\n",
       "       [1.2766710e-02, 3.6938712e-02, 4.6631144e-04, ..., 3.1692013e-02,\n",
       "        1.0604068e-03, 8.8037336e-03],\n",
       "       [1.0493499e-01, 1.3349617e-01, 1.8987173e-02, ..., 4.9183499e-02,\n",
       "        2.8089635e-02, 1.0951228e-01]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.predict(val_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 0.0\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8e152902236d0ddc17cab15fa4740f8c30870419133c500d5ccb5dc7eb029fd5"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
